{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8ebc590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, pathlib, random, time\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fdc686f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lists.json') as f:\n",
    "    j = json.load(f)\n",
    "\n",
    "target_list = j['target']\n",
    "guess_list = j[\"guess\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9f9b140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_freq(lst):\n",
    "    hist = defaultdict(int)\n",
    "    for word in lst:\n",
    "        for char in word:\n",
    "            hist[char] += 1\n",
    "    mx = max(hist.values())\n",
    "    for char in hist:\n",
    "        hist[char] /= mx\n",
    "    return hist\n",
    "\n",
    "def print_char_freq(cf):\n",
    "    for char in sorted(list(cf.keys())):\n",
    "        print(f'{char}: {cf[char]}')\n",
    "        \n",
    "def freq_score(word, cf):\n",
    "    return sum(cf[x] for x in word) / len(word) \n",
    "\n",
    "def uniq_score(word):\n",
    "    return (len(word) - len(set(word))) / (len(word) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef2c056b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cf = char_freq(target_list)\n",
    "#print_char_freq(cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0ef9d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stime\n",
      "vouch\n"
     ]
    }
   ],
   "source": [
    "print(random.choice(guess_list))\n",
    "print(random.choice(target_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f251bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg = pd.DataFrame([[w, freq_score(w, cf), uniq_score(w), 1.0] for w in guess_list], columns=['word', 'freq_score', 'uniq_score', 'is_guess_word'])\n",
    "dft = pd.DataFrame([[w, freq_score(w, cf), uniq_score(w), 0.0] for w in target_list], columns=['word', 'freq_score', 'uniq_score', 'is_guess_word'])\n",
    "df = dfg.append(dft)\n",
    "df.set_index('word', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edaa175d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "freq_score       0.725547\n",
       "uniq_score       1.000000\n",
       "is_guess_word    1.000000\n",
       "Name: esses, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[2801]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a94b0d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def word_to_action(word, guesses, history):\n",
    "    return dfword_to_action((word, df.loc[word]), guesses, history)\n",
    "    \n",
    "def dfword_to_action(dfword, guesses, history):\n",
    "    #the action is going to be a word that we will submit next\n",
    "    #for the purposes of feeding into the model, we will represent the action word as:\n",
    "    #  how many of the entries in the hint history this word conforms to\n",
    "    #  how many untried letters it gives us\n",
    "    #  the number of uniq letters in the word\n",
    "    #  the frequency of the letters in the word\n",
    "    #  whether or not the word is in the guess list (as opposed to the target list)\n",
    "    word = dfword[0]\n",
    "    dfword = dfword[1]\n",
    "    \n",
    "    if guesses:\n",
    "        conforms_to_history = sum([int(validate_against_hint(word,g,history[i])) for i,g in enumerate(guesses)]) / len(guesses)\n",
    "    else: # we haven't made any guess yet, so this must conform\n",
    "        conforms_to_history = 1.0\n",
    "    num_untried_letters = len(set(word) - set(''.join(guesses))) / 5 #normalise to 1\n",
    "    action = np.array([conforms_to_history, num_untried_letters, dfword['freq_score'], dfword['uniq_score'], dfword['is_guess_word']])\n",
    "    \n",
    "    #if word == 'aargh':\n",
    "    #    print(f'recons', action, history, guesses)\n",
    "    return action\n",
    "        \n",
    "def update_action(action, word, history, guesses):\n",
    "    \n",
    "    #just call validate_against_hint() for the most recent hint\n",
    "    #print(guesses.__class__, guesses)  \n",
    "    #print(word.__class__, word)\n",
    "\n",
    "    num_untried_letters = len(set(word) - set(''.join(guesses))) / 5\n",
    "    conforms_to_history = (action[0] * (len(guesses)-1) + int(validate_against_hint(word,guesses[-1],history[-1])) ) / len(guesses)\n",
    "    #if word == 'aargh':\n",
    "    #    print(f'updaten', action, history, guesses)\n",
    "    return np.array([conforms_to_history, num_untried_letters, action[2], action[3], action[4]])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "895b599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def construct_actions_global(arg): #guesses, history, start_idx, end_idx):\n",
    "    st = time.time()\n",
    "    guesses, history, start_idx, end_idx = arg\n",
    "    #print(guesses, history, start_idx, end_idx)\n",
    "    ret = np.array([dfword_to_action(dfword, guesses, history) for dfword in df.iloc[start_idx:end_idx].iterrows()])\n",
    "    #print(f'construct_actions_global took {time.time() - st}')\n",
    "    return ret\n",
    "    \n",
    "def update_actions_global(arg): \n",
    "    st = time.time()\n",
    "    start_idx, end_idx = arg\n",
    "    #print(guesses, history, start_idx, end_idx)\n",
    "    ret = np.array([update_action(env_global.actions[i], df.iloc[i].name,  env_global.history, env_global.guesses) for i in range(start_idx, end_idx, 1)])\n",
    "    #print(f'update_actions_global took {time.time() - st}')\n",
    "    return ret\n",
    "    \n",
    "\n",
    "class ActionSpace:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "    \n",
    "    \n",
    "class Env:\n",
    "    def __init__(self, df, target_word=None):\n",
    "        self.df = df\n",
    "        self.specified_target_word = False\n",
    "        if target_word:\n",
    "            self.specified_target_word = True\n",
    "            self.target = target_word            \n",
    "            \n",
    "        self.reset()     \n",
    "        self.num_letters = len(self.target)\n",
    "        self.num_guesses = 6\n",
    "        \n",
    "        self.num_processes = mp.cpu_count() - 1\n",
    "        self.action_space = ActionSpace(len(self.df))\n",
    "       \n",
    "        \n",
    "    def index_from_word(self, word):\n",
    "        return self.df.index.get_loc(word)\n",
    "    \n",
    "    def word_from_index(self, idx):\n",
    "        return self.df.iloc[idx].name\n",
    "    \n",
    "    def submit_guess(self, guess):\n",
    "        wrongplace = [0] * len(self.target)\n",
    "        hints = np.zeros(len(self.target))\n",
    "        rightplace = [guess[n] == chrt for n,chrt in enumerate(self.target)]\n",
    "        #print(f'comparing {guess} against {self.target}.  rightplace {rightplace}')\n",
    "        \n",
    "        for n,chrt in enumerate(self.target):\n",
    "            if rightplace[n] == 1: continue #this character has already been scored, skip it\n",
    "            for m,chrg in enumerate(guess):\n",
    "                if n == m: continue # we've already checked rightplace matches above\n",
    "                if chrt != chrg: continue\n",
    "                if wrongplace[m] == 1: continue\n",
    "                if rightplace[m] == 1: continue\n",
    "                \n",
    "                wrongplace[m] = 1\n",
    "                break\n",
    "\n",
    "        for i in range(len(self.target)):\n",
    "            hints[i] = 2 if rightplace[i] == 1 else wrongplace[i]\n",
    "        \n",
    "        return hints\n",
    "    \n",
    "    def reset(self):\n",
    "        self.history = np.array([[]])\n",
    "        self.guesses = []\n",
    "        if not self.specified_target_word:\n",
    "            self.target = df[df['is_guess_word'] == 0.0].sample().iloc[0].name\n",
    "        self.actions = None\n",
    "            \n",
    "    def construct_actions(self):\n",
    "        return np.array([dfword_to_action(dfword, self.guesses, self.history) for dfword in self.df.iterrows()])\n",
    "    \n",
    "    def update_actions(self):\n",
    "        return np.array([update_action(self.actions[i], df.iloc[i].name, self.history, self.guesses) for i in range(len(self.actions))])\n",
    "    \n",
    "    def construct_actions_mp(self):\n",
    "        \n",
    "        grp_lst_args = []\n",
    "        grp_guesses = [self.guesses] * self.num_processes\n",
    "        grp_history = [self.history] * self.num_processes\n",
    "        \n",
    "        chunk_size = int(len(self.df) / self.num_processes) + 1\n",
    "        start_offsets = list(range(0, len(self.df), chunk_size))\n",
    "        end_offsets = start_offsets[1:] + [len(self.df)]\n",
    "        grp_lst_args = list(zip(grp_guesses, grp_history, start_offsets, end_offsets))\n",
    "        \n",
    "        #print(grp_lst_args)\n",
    "        self.pool = mp.Pool(processes=self.num_processes)\n",
    "        results = self.pool.map(construct_actions_global, grp_lst_args)\n",
    "        self.pool.close()\n",
    "        self.pool.join()\n",
    "        return np.concatenate(results)\n",
    "    \n",
    "    def update_actions_mp(self):\n",
    "        global env_global\n",
    "        env_global = self\n",
    "        \n",
    "        grp_lst_args = []\n",
    "        \n",
    "        chunk_size = int(len(self.df) / self.num_processes) + 1\n",
    "        start_offsets = list(range(0, len(self.df), chunk_size))\n",
    "        end_offsets = start_offsets[1:] + [len(self.df)]\n",
    "        grp_lst_args = list(zip(start_offsets, end_offsets))\n",
    "        \n",
    "        #print(grp_lst_args)\n",
    "        self.pool = mp.Pool(processes=self.num_processes)\n",
    "        results = self.pool.map(update_actions_global, grp_lst_args)\n",
    "        self.pool.close()\n",
    "        self.pool.join()\n",
    "        return np.concatenate(results)\n",
    "    \n",
    "    def construct_state(self):\n",
    "        #print(history)\n",
    "        #so the state is going to be:\n",
    "            #  The number of green locations we know\n",
    "            #  The number of other letters we know to be in the word\n",
    "            #  The sequence number of the guess (1st guess, 2nd guess etc.)\n",
    "\n",
    "        #the number of locations which were green at some point in the history\n",
    "        num_green_locs = np.count_nonzero(self.history.max(axis=0) == 2)\n",
    "\n",
    "        green_chars = [self.guesses[x][y] for x,y in np.argwhere(self.history == 2) ]\n",
    "        orange_chars = [self.guesses[x][y] for x,y in np.argwhere(self.history == 1) ]\n",
    "        black_chars = [self.guesses[x][y] for x,y in np.argwhere(self.history == 0) ]\n",
    "        num_other_letters = len(set(orange_chars) - set(green_chars))\n",
    "        num_black_letters = len(set(black_chars))\n",
    "\n",
    "        sequence_number = int(self.history.size / 5)\n",
    "        #print(f'construct_state() with seqno {sequence_number}')\n",
    "\n",
    "        sequence_number_onehot = np.zeros(self.num_guesses)\n",
    "        sequence_number_onehot[sequence_number] = 1.0\n",
    "        return np.concatenate((np.array([num_green_locs, num_other_letters, num_black_letters])/5, sequence_number_onehot))\n",
    "\n",
    "    def step_by_index(self, guess_idx):\n",
    "        return self.step(self.word_from_index(guess_idx))\n",
    "    \n",
    "    \n",
    "    def step(self, guess, reconstruct=False): #returns state, reward, done, actions\n",
    "        #print(actions)\n",
    "        hints = self.submit_guess(guess)\n",
    "\n",
    "        #print(list(zip(self.guesses,self.history)))\n",
    "        if self.history.size == 0:\n",
    "            self.history = np.expand_dims(hints,0)\n",
    "            best_hints = 0\n",
    "        else:\n",
    "            best_hints = np.apply_along_axis(np.sum, 1, self.history).max()\n",
    "            self.history = np.row_stack([self.history, hints])\n",
    "            \n",
    "        #print(f'======={guess} ({self.target}) => {hints}= {best_hints} =======')\n",
    "        \n",
    "        self.guesses.append(guess)\n",
    "        reward = max(0, hints.sum() - best_hints)\n",
    "        done = (hints.sum() == self.num_letters * 2 or len(self.guesses) == self.num_guesses)\n",
    "    \n",
    "        if not done:\n",
    "            state = self.construct_state()\n",
    "            if self.actions is None or reconstruct:\n",
    "                st = time.time()\n",
    "                self.actions = self.construct_actions_mp() #slower\n",
    "                #print(f'construct actions took {time.time() - st}')\n",
    "                #print(f'after recons {self.actions[2]}')\n",
    "            else:\n",
    "                st = time.time()\n",
    "                self.actions = self.update_actions_mp() #faster\n",
    "                #print(f'update actions took {time.time() - st}')\n",
    "                #print(f'after update {self.actions[2]}')\n",
    "        else:\n",
    "            state = None\n",
    "            self.actions = None\n",
    "        return state, reward, done, self.actions\n",
    "\n",
    "    \n",
    "def hint_to_hinty(hint):\n",
    "    #hint takes form [0,1,2,1,0]\n",
    "    #hinty takes form {2:[2], 1:[1,3], 0:[0,4]}\n",
    "    hinty = {}\n",
    "    for n in [0,1,2]:\n",
    "        hinty[n] = [i for i, x in enumerate(hint) if x == n]\n",
    "    #print(f'hint_to_hinty() {hint}, {hinty}')\n",
    "    return hinty\n",
    "    \n",
    "def validate_against_hint(word, guess, hint):\n",
    "    return validate_against_hinty(word, guess, hint_to_hinty(hint))\n",
    "\n",
    "def validate_against_hinty(word, guess, hinty):\n",
    "    #hinty takes form {2:[idx,..], 1:[idx,..], 0:[idx,..]}\n",
    "    #print(hinty)\n",
    "    for idx in hinty[2]: # check the fixed letters first\n",
    "        if word[idx] != guess[idx]:\n",
    "            return False\n",
    "      \n",
    "    for idx in hinty[0]:\n",
    "        #get the number of times char appears in target word (minus the times it appears in the correct location)\n",
    "        indices = [i for i,x in enumerate(word) if x == guess[idx] and i not in hinty[2]]\n",
    "        #get number of times char appears in guess word in the wrong location\n",
    "        indices_g = [n for n,x in enumerate(guess) if x == guess[idx] and n in hinty[1]]\n",
    "        #we already know that there is one not-exist hint for this char, so\n",
    "        #if there are more fewer wrong location hints for this letter than there are actual occurrences of the letter\n",
    "        #then the hint does not validate against this word\n",
    "        if len(indices) > len(indices_g):\n",
    "            return False\n",
    "    for idx in hinty[1]:\n",
    "        if word[idx] == guess[idx]:\n",
    "            return False\n",
    "        #get all the indices of the character in the target word\n",
    "        #print(word.__class__, word)\n",
    "        indices = [i for i,x in enumerate(word) if x == guess[idx] and i not in hinty[2]]\n",
    "        #remove all the indices where there is already a fixed position hint\n",
    "        \n",
    "        #now count all the occurences of the char in guess where the location is wrong\n",
    "        indices_g = [i for i,x in enumerate(guess) if x == guess[idx] and i in hinty[1]]\n",
    "        #if there are more wrong loc hints for this char than there are actual occurrences, then it must be bogus\n",
    "        if len(indices) < len(indices_g):\n",
    "            return False\n",
    "    return True            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "928c0f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "update 1.2987322807312012\n",
      "==============================\n",
      "recons 1.2577588558197021\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-1e10eddf810b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'recons {time.time() - st}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions_update\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mactions_recons\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'bool' object has no attribute 'all'"
     ]
    }
   ],
   "source": [
    "e = Env(df, target_word='crapy')\n",
    "e.reset()\n",
    "print(e.num_processes)\n",
    "#e.num_processes = 1\n",
    "words = ['beast', 'treat', 'pzazz', 'jobby', 'bobby', 'jimbo']\n",
    "actions_update = []\n",
    "actions_recons = []\n",
    "\n",
    "st = time.time()\n",
    "for i, word in enumerate(words):\n",
    "    actions_update.append(e.step(word, reconstruct=False)[3])\n",
    "print(f'update {time.time() - st}')\n",
    "    \n",
    "print('==============================')\n",
    "e.reset()\n",
    "st =time.time()\n",
    "for i, word in enumerate(words):\n",
    "    actions_recons.append(e.step(word, reconstruct=True)[3])\n",
    "    \n",
    "    \n",
    "print(f'recons {time.time() - st}')    \n",
    "for n in range(len(words)):\n",
    "    print((actions_update[n] == actions_recons[n]).all())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bef5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = Env(df)\n",
    "e.reset()\n",
    "st = time.time()\n",
    "rmp = e.construct_actions_mp()\n",
    "print(time.time() - st)\n",
    "e.reset()\n",
    "st = time.time()\n",
    "r = e.construct_actions()\n",
    "print(time.time() - st)\n",
    "\n",
    "print(r.__class__)\n",
    "print(r.shape)\n",
    "\n",
    "print(rmp.__class__)\n",
    "print(rmp.shape)\n",
    "\n",
    "print((r == rmp).all())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14164929",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_simple = Env(target_list, target_word='abcde')\n",
    "tests_simple = {'abcde': [2,2,2,2,2],\n",
    "         'acbde': [2,1,1,2,2],\n",
    "         'azcde': [2,0,2,2,2],\n",
    "         'aacde': [2,0,2,2,2],\n",
    "         'zacde': [0,1,2,2,2],\n",
    "         'zzdzz': [0,0,1,0,0],\n",
    "         'zzddz': [0,0,0,2,0],\n",
    "         'zdddz': [0,0,0,2,0],\n",
    "         'ddddd': [0,0,0,2,0],\n",
    "         'zzzdd': [0,0,0,2,0],\n",
    "         'zzdez': [0,0,1,1,0]}\n",
    "\n",
    "e_repeat = Env(target_list, target_word='abcae')\n",
    "tests_repeat = {'abcde': [2,2,2,0,2],\n",
    "         'acbde': [2,1,1,0,2],\n",
    "         'azcde': [2,0,2,0,2],\n",
    "         'aacde': [2,1,2,0,2],\n",
    "         'zacde': [0,1,2,0,2],\n",
    "         'zzdzz': [0,0,0,0,0],\n",
    "         'zzddz': [0,0,0,0,0],\n",
    "         'zdddz': [0,0,0,0,0],\n",
    "         'ddddd': [0,0,0,0,0],\n",
    "         'zzzdd': [0,0,0,0,0],\n",
    "         'zzdez': [0,0,0,1,0],\n",
    "         'aaaaa': [2,0,0,2,0],\n",
    "         'aaaza': [2,1,0,0,0],\n",
    "         'zaazz': [0,1,1,0,0],\n",
    "         'zaaza': [0,1,1,0,0]}\n",
    "\n",
    "for e,tests in [(e_simple, tests_simple),(e_repeat, tests_repeat)]:\n",
    "    for guess,expected in tests.items():\n",
    "        #guess = random.choice(guess_list + target_list)\n",
    "        actual = e.submit_guess(guess)\n",
    "        hinty = hint_to_hinty(expected)\n",
    "        hinty_valid = validate_against_hinty(e.target, guess, hinty)\n",
    "        print(e.target, guess, actual, expected, expected == actual, hinty_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b83231",
   "metadata": {},
   "outputs": [],
   "source": [
    "e=Env(df)\n",
    "for _ in range(10):\n",
    "    n = random.randint(0, len(e.df))\n",
    "    w = e.word_from_index(n)\n",
    "    n_ = e.index_from_word(w)\n",
    "    print(f'{n}, {w}, {n_}')\n",
    "    assert(n == n_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca16daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_guess(guess_list, target_list):\n",
    "    guess_idx = random.randint(0, len(guess_list) + len(target_list))\n",
    "    is_guess = guess_idx < len(guess_list)\n",
    "    if is_guess:\n",
    "        word = guess_list[guess_idx]\n",
    "    else:\n",
    "        word = target_list[guess_idx - len(guess_list)]\n",
    "    return word, is_guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9248df4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'beast'\n",
    "e = Env(df, target_word='beast')\n",
    "e.step('treat')\n",
    "#e.guesses = ['treat']\n",
    "#e.history = np.array([[0.0, 0.0, 1.0, 1.0, 2.0]])\n",
    "#Env(target_list, target_word='beast').submit_guess('treat')\n",
    "print(e.guesses, e.history)\n",
    "actual = e.construct_state()\n",
    "expected = [0.2, 0.4, 0.2]\n",
    "print(expected, actual, expected == actual)\n",
    "\n",
    "actual = word_to_action('feast', ['treat'], np.array([[0.0, 0.0, 1.0, 1.0, 2.0]]))\n",
    "expected = [1.0, 0.4, 0.62287105, 0.0, 0.0]\n",
    "print(expected, actual, expected == actual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650e6062",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_guesses = 6\n",
    "e = Env(df)\n",
    "\n",
    "print(e.target)\n",
    "num_letters = len(e.target)\n",
    "history = np.array([[]])\n",
    "guesses = []\n",
    "rewards = []\n",
    "for i in range(num_guesses):\n",
    "    #guess, is_guess_list = random_guess(guess_list, target_list)\n",
    "    actions = e.construct_actions_mp()\n",
    "    state = e.construct_state()\n",
    "    #here feed it into a model to choose the word\n",
    "    #guess, value = np.argmax(model(state)) # but do this epsilon greedy\n",
    "    \n",
    "    #print(actions)\n",
    "    hints = e.submit_guess(guess)\n",
    "    \n",
    "    print(f'======={guess}========')\n",
    "    print(list(zip(guesses,history)))\n",
    "    if history.size == 0:\n",
    "        history = np.expand_dims(hints,0)\n",
    "    else:\n",
    "        history = np.row_stack([history, hints])\n",
    "    guesses.append(guess)\n",
    "    if hints.sum() == num_letters * 2 or i == num_guesses - 1:\n",
    "        reward = hints.sum()\n",
    "        done = True\n",
    "    else:\n",
    "        reward = -1\n",
    "        done = False\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "#so the state is going to be:\n",
    "#  The number of green locations we know\n",
    "#  The number of other letters we know to be in the word\n",
    "#  The sequence number of the guess (1st guess, 2nd guess etc.)\n",
    "\n",
    "#the action is going to be a word that we will submit next\n",
    "#for the purposes of feeding into the model, we will represent the action word as:\n",
    "#  whether or not it conforms to the hint history\n",
    "#  how many new letters it gives us\n",
    "#  the number of uniq letters in the word\n",
    "#  the frequency of the letters in the word\n",
    "\n",
    "#the reward is going to be:\n",
    "#  -1 on all states except the last one\n",
    "#  on the last state (which can either be after guess 6 or on guessing the correct word):\n",
    "#    the sum of the last hint (ie. 2 for a correct letter/position combo, 1 for a letter in the wrong place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dd99de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "#plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832e7980",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def clear(self):\n",
    "        self.memory.clear()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9ed4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get number of actions from gym action space\n",
    "#n_actions = env.action_space.n\n",
    "n_action_features = 5\n",
    "n_state_features = 9\n",
    "n_input_features = n_action_features + n_state_features\n",
    "\n",
    "\n",
    "def select_action(policy_net, state, actions, eps_threshold):\n",
    "    sample = random.random()\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            #now combine the state (shape 3,) and action (shape 5, n) into one input array (shape 8,n)\n",
    "            #first expand the state so that it is shape 3,1\n",
    "            #then repeat it to 3,n\n",
    "            states = np.repeat(np.expand_dims(state, 0), actions.shape[0], axis=0)\n",
    "            #print(f'states shape {states.shape} actions shape {actions.shape}')\n",
    "            #then concatenate to 8,n\n",
    "            state_actions = np.concatenate((states, actions), axis=1)\n",
    "            # policy_net(state_action) will return a single value estimate for each state/action row\n",
    "            # so, probably shape (1,n)\n",
    "            # Then return the index which has the max value\n",
    "            \n",
    "            estimate = policy_net(torch.tensor(state_actions, device=device, dtype=torch.float))\n",
    "            #print(f'ESTIMATE>>>{estimate.__class__} {estimate.shape} {estimate} {estimate.max(0).indices.item()}<<<')\n",
    "            return estimate.max(0).indices.item()\n",
    "    else:\n",
    "        randindex = random.randrange(len(actions))\n",
    "        print(f'returning random index {randindex}')\n",
    "        return randindex #torch.tensor([[randindex]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "\n",
    "def plot_values(vals, axes=['duration', 'episode']):\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel(axes[1])\n",
    "    plt.ylabel(axes[0])\n",
    "    plt.plot(np.array(vals))\n",
    "    # Take 20 episode averages and plot them too\n",
    "    window_width = 20\n",
    "    if len(vals) >= window_width:\n",
    "        cumsum_vec = np.cumsum(np.insert(vals, 0, 0)) \n",
    "        ma_vec = (cumsum_vec[window_width:] - cumsum_vec[:-window_width]) / window_width\n",
    "        plt.plot(np.insert(ma_vec, 0, [None]*int(window_width/2)))\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    #if is_ipython:\n",
    "    #    display.clear_output(wait=True)\n",
    "    #    display.display(plt.gcf())\n",
    "    \n",
    "def plot_all(episode_durations, episode_rewards, losses, epsilons, gammas):\n",
    "    plot_values(episode_durations, axes=['duration', 'episode'])\n",
    "    plot_values(episode_rewards, axes=['reward', 'episode'])\n",
    "    if losses: plot_values(losses, axes=['loss', 'step'])\n",
    "    if epsilons: plot_values(epsilons, axes=['epsilon', 'step'])\n",
    "    if gammas: plot_values(gammas, axes=['gamma', 'step'])\n",
    "    #plt.ioff()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae28e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, inputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(inputs, 20)\n",
    "        self.fc2 = nn.Linear(20, 16)\n",
    "        self.fc3 = nn.Linear(16, 20)\n",
    "        self.head = nn.Linear(20, 1)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c642286",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearQ(nn.Module):\n",
    "\n",
    "    def __init__(self, inputs):\n",
    "        super(LinearQ, self).__init__()\n",
    "        self.head = nn.Linear(inputs, 1)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547e3d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Env(df)\n",
    "class NaiveModel():\n",
    "    def __init__(self, startword_idx=None, target_list_only=False):\n",
    "        self.startword_idx = startword_idx\n",
    "        self.target_list_only = target_list_only #when guessing, only choose from the target list (not the full guess list)\n",
    "        \n",
    "    def guess(self, idx, state_action):\n",
    "        if state_action[2] == 0: #if this is the first guess\n",
    "            return \n",
    "    \n",
    "    def conforms_to_history(state_action):\n",
    "        return state_action[n_state_features] == 1.0\n",
    "    \n",
    "    def is_in_target_list(state_action):\n",
    "        return state_action[n_state_features+4] == 0.0\n",
    "                \n",
    "    def __call__(self, x): # we must return the value of each action\n",
    "        #x will be a batch\n",
    "        # if this is the first guess, we must return 1 for the chosen startword\n",
    "        print(f'startword analysis {x[0]} {x[0].__class__}')\n",
    "        if x[0][3] == 1.0: #if this is the first guess\n",
    "            if self.startword_idx: # if we have specified a starting word\n",
    "                choice = self.startword_idx\n",
    "                print(f'choosing fixed startword at {choice}')\n",
    "            else:  #chose a random startword\n",
    "                choice = random.randint(0,len(x))\n",
    "                print(f'choosing random word at {choice}')\n",
    "        else:\n",
    "            #choose a random startword from the words that conform to the history\n",
    "            \n",
    "            conformant_indices = [idx for idx, state_action in enumerate(x) if NaiveModel.conforms_to_history(state_action)]\n",
    "            if self.target_list_only:\n",
    "                conformant_indices = [idx for idx in conformant_indices if NaiveModel.is_in_target_list(x[idx])]\n",
    "            choice = random.sample(conformant_indices, 1)[0]\n",
    "            print(f'choosing conformant word at {choice} from {len(conformant_indices)} conformant words')\n",
    "\n",
    "        print(f'word at {choice} is {env.word_from_index(choice)}')\n",
    "        ret = torch.zeros(len(x))\n",
    "        ret[choice] = 1.0\n",
    "        return ret\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "    def to(self, device):\n",
    "        return self\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c1200d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(model, optimizer, memory, batch_size=128):\n",
    "\n",
    "    transitions = memory.sample(batch_size)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    state_batch = np.stack([tr.state for tr in transitions])\n",
    "    action_batch = np.stack([tr.action for tr in transitions])\n",
    "      \n",
    "    reward_batch = np.stack([tr.reward for tr in transitions])\n",
    "    state_action_batch = np.concatenate((state_batch, action_batch), axis=1)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_value_estimates = model(torch.tensor(state_action_batch, device=device, dtype=torch.float))\n",
    "    #print(f'ESTIMATE>>>{estimate.__class__} {estimate.shape} {estimate} {estimate.max(0).indices.item()}<<<')\n",
    "       \n",
    "    expected_state_action_values = torch.tensor(reward_batch, device=device, dtype=torch.float)\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_value_estimates, expected_state_action_values)\n",
    "    \n",
    "    print(f'loss {loss}')\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in model.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n",
    "\n",
    "class TrainConfig():\n",
    "    def __init__(self, train_interval=128, batch_size=128, clear_memory=False, lr=0.01):\n",
    "        self.train_interval = train_interval\n",
    "        self.batch_size = batch_size\n",
    "        self.clear_memory = clear_memory\n",
    "        self.lr = lr\n",
    "        \n",
    "class ValueConfig():\n",
    "    def __init__(self, name='reward', gamma=[0.9, 0.05, 200]):\n",
    "        self.name = name\n",
    "        self.gamma = gamma\n",
    "        \n",
    "class ModelConfig():\n",
    "    def __init__(self, name='naive', startword=None, target_list_only=None):\n",
    "        self.name = name\n",
    "        self.startword = startword\n",
    "        self.target_list_only = target_list_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c62073",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "def run_experiment(model=ModelConfig(name='naive', startword=None, target_list_only=False),\n",
    "                   num_episodes=128,\n",
    "                   eps=[0.9, 0.05, 200],\n",
    "                   value_function=ValueConfig(name='reward',gamma=[0.0, 1.0, 200]),\n",
    "                   training=TrainConfig(clear_memory=False, batch_size=128, train_interval=128)):\n",
    "    torch.manual_seed(0)\n",
    "    random.seed(0)\n",
    "    np.random.seed(0)\n",
    "    EPS_START = eps[0]\n",
    "    EPS_END = eps[1]\n",
    "    EPS_DECAY = eps[2]\n",
    "    GAMMA_START, GAMMA_END, GAMMA_DECAY = value_function.gamma\n",
    "    env = Env(df)\n",
    "    memory = ReplayMemory(10000)\n",
    "    starting_actions = env.construct_actions()\n",
    "    starting_state = env.construct_state()\n",
    "\n",
    "    steps_done = 0\n",
    "    last_training = 0\n",
    "    losses = []\n",
    "    episode_rewards = []\n",
    "    episode_durations = []\n",
    "    epsilons = []\n",
    "    gammas = []\n",
    "    \n",
    "    if model.name =='naive':\n",
    "        startword_idx = env.index_from_word(model.startword) if model.startword else None\n",
    "        \n",
    "        policy_net = NaiveModel(startword_idx=startword_idx, target_list_only=model.target_list_only)\n",
    "        optimizer = None\n",
    "        EPS_START = 0\n",
    "        EPS_END = 0\n",
    "    elif model.name == 'linear':\n",
    "        policy_net = LinearQ(n_input_features).to(device)\n",
    "        optimizer = optim.RMSprop(policy_net.parameters(), lr=training.lr)\n",
    "    else:\n",
    "        policy_net = DQN(n_input_features).to(device)\n",
    "        optimizer = optim.RMSprop(policy_net.parameters(), lr=training.lr)\n",
    "\n",
    "    print(f'pn params {list(policy_net.parameters())}')\n",
    "    for i_episode in range(num_episodes):\n",
    "        # Initialize the environment and state\n",
    "        env.reset()\n",
    "        print(f'=========================episode {i_episode} {env.target}======================')\n",
    "\n",
    "        episode_memory = []\n",
    "        state = starting_state\n",
    "        actions = starting_actions\n",
    "        for t in count():\n",
    "            eps = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "            GAMMA = GAMMA_END + (GAMMA_START - GAMMA_END) * math.exp(-1. * steps_done / GAMMA_DECAY)\n",
    "            epsilons.append(eps)\n",
    "            gammas.append(GAMMA)\n",
    "            steps_done += 1\n",
    "            # Select and perform an action\n",
    "            #print(state, actions)\n",
    "            action_idx = select_action(policy_net, state, actions, eps)\n",
    "            selected_action = actions[action_idx]\n",
    "            print(f'------guess {t} {action_idx} {env.word_from_index(action_idx)} {selected_action}-------')\n",
    "            next_state, reward, done, actions = env.step_by_index(action_idx)\n",
    "            print(f'reward {reward} done {done} ')\n",
    "            reward = np.array([reward])\n",
    "\n",
    "            # Store the transition in memory\n",
    "            #memory.push(state, selected_action, reward)\n",
    "            episode_memory.append([state, selected_action, reward])\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                episode_durations.append(t + 1)\n",
    "                episode_reward = sum([tr[2] for tr in episode_memory])\n",
    "                print(f'episode {i_episode} finished.  reward {episode_reward}  eps {eps}  gamma {GAMMA}  steps {steps_done}  memory {len(memory)}')\n",
    "                episode_rewards.append(episode_reward)\n",
    "                if value_function.name == 'reward':\n",
    "                    for tr in episode_memory:\n",
    "                        memory.push(*tr)\n",
    "                elif value_function.name == 'hybrid':\n",
    "                    #use the returned reward,\n",
    "                    #but reduce it if we didn't get the target word by the end of the episode\n",
    "                    #and increase it if we took less than 6 guesses\n",
    "                    if episode_reward == 10.0: # if we got the target word\n",
    "                        #apply a positive factor to all guess values\n",
    "                        bonus = (env.num_guesses - (t+1)) * GAMMA\n",
    "                        # after 3 guesses offset = 3* GAMMA\n",
    "                        # after 6 guesses, offset = 0 * GAMMA \n",
    "                    else:\n",
    "                        bonus = -2*GAMMA\n",
    "                    \n",
    "                    print(f'original rewards {[tr[2] for tr in episode_memory]}')\n",
    "                    for tr in episode_memory:\n",
    "                        q = max(0.0, tr[2][0] + bonus)\n",
    "                        tr[2][0] = q\n",
    "                        memory.push(*tr)\n",
    "                    print(f'hybrid rewards {[tr[2] for tr in episode_memory]}')    \n",
    "                elif value_function.name == 'discounted':\n",
    "                    #q is the actual value of the state_action value function\n",
    "                    # which is the discounted reward.\n",
    "                    #on the last guess q(n) is equal to the total episode reward \n",
    "                    #and q(n-1) is equal to -1 + episode_reward * GAMMA.\n",
    "                    #and q(n-2) = -1 + (n-1) * GAMMA\n",
    "                    #min(q) = -6 (for GAMMA = 1)\n",
    "                    #max(q) = 10\n",
    "                    q = episode_reward\n",
    "                    qs = []\n",
    "                    if episode_reward < 10: q = q * 0.7 #reduce the reward if we didn't get the correct answer\n",
    "                    for idx,tr in enumerate(reversed(episode_memory)):\n",
    "                        if idx > 0:\n",
    "                            q = -1 + GAMMA * q\n",
    "                        memory.push(tr[0], tr[1], (q + 6)) # add 6 on to the value so that it is never < 0 \n",
    "                        qs.append(q+6)\n",
    "                    print(f'discounted rewards {list(reversed(qs))} vs. {[tr[2] for tr in episode_memory]}')\n",
    "                else:\n",
    "                    raise Exception(f'bad value function {value_function.name}')\n",
    "\n",
    "                # If we have gathered enough data, Perform one step of the optimization (on the policy network)\n",
    "                if model.name != 'naive' \\\n",
    "                    and len(memory) >= training.batch_size \\\n",
    "                    and steps_done - last_training > training.train_interval:\n",
    "                    loss = optimize_model(policy_net, optimizer, memory, batch_size=training.batch_size)\n",
    "                    losses.append(loss)\n",
    "                    if training.clear_memory: memory.clear()\n",
    "                    last_training = steps_done\n",
    "                #plot_durations()\n",
    "                break\n",
    "\n",
    "    print('Complete')\n",
    "    \n",
    "    return episode_durations, episode_rewards, losses, epsilons, gammas\n",
    "\n",
    "#env.render()\n",
    "#env.close()\n",
    "#plt.ioff()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d33ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all(*run_experiment(\n",
    "    model={'name': 'naive', 'startword': None, 'target_list_only':False},\n",
    "    num_episodes=64\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960f42ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all(*run_experiment(\n",
    "    model={'name': 'naive', 'startword': None, 'target_list_only':True},\n",
    "    num_episodes=64\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f13c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all(*run_experiment(\n",
    "    model={'name': 'naive', 'startword': 'roate', 'target_list_only':True},\n",
    "    num_episodes=64\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d04d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all(*run_experiment(\n",
    "    model={'name': 'naive', 'startword': 'roate', 'target_list_only':False},\n",
    "    num_episodes=64\n",
    "    ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94916dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all(*run_experiment(\n",
    "    model=ModelConfig(name='linear'),\n",
    "    value_function=ValueConfig(name='hybrid', gamma=[0.0, 0.0, 200]),\n",
    "    eps=[0.0, 0.0, 400],\n",
    "    num_episodes=150,\n",
    "    training=TrainConfig(train_interval=16, batch_size=64, clear_memory=False, lr=0.07)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bbee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all(*run_experiment(\n",
    "    model=ModelConfig(name='linear'),\n",
    "    value_function=ValueConfig(name='hybrid', gamma=[0.0, 0.0, 200]),\n",
    "    eps=[0.0, 0.0, 400],\n",
    "    num_episodes=150,\n",
    "    training=TrainConfig(train_interval=16, batch_size=64, clear_memory=False, lr=0.03)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754258f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all(*run_experiment(\n",
    "    model=ModelConfig(name='linear'),\n",
    "    value_function=ValueConfig(name='hybrid', gamma=[0.0, 0.0, 200]),\n",
    "    eps=[0.0, 0.0, 400],\n",
    "    num_episodes=150,\n",
    "    training=TrainConfig(train_interval=16, batch_size=64, clear_memory=False, lr=0.1)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d116eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all(*run_experiment(\n",
    "    model=ModelConfig(name='linear'),\n",
    "    value_function=ValueConfig(name='hybrid', gamma=[0.3, 0.3, 200]),\n",
    "    eps=[0.0, 0.0, 400],\n",
    "    num_episodes=150,\n",
    "    training=TrainConfig(train_interval=16, batch_size=64, clear_memory=False, lr=0.07)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1b6a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all(*run_experiment(\n",
    "    model=ModelConfig(name='linear'),\n",
    "    value_function=ValueConfig(name='hybrid', gamma=[0.6, 0.6, 200]),\n",
    "    eps=[0.0, 0.0, 400],\n",
    "    num_episodes=150,\n",
    "    training=TrainConfig(train_interval=16, batch_size=64, clear_memory=False, lr=0.07)\n",
    "    ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
