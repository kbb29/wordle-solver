{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8ebc590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, pathlib, random, time\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "\n",
    "from environment import Env, validate_against_hint, load_word_lists, construct_word_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae1b8e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = construct_word_df(*load_word_lists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "895b599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def construct_state_tensor(guesses, history):\n",
    "        #print(history)\n",
    "        #so the state is going to be:\n",
    "            #  The number of green locations we know\n",
    "            #  The number of other letters we know to be in the word\n",
    "            #  The sequence number of the guess (1st guess, 2nd guess etc.)\n",
    "\n",
    "        #the number of locations which were green at some point in the history\n",
    "        num_green_locs = np.count_nonzero(history.max(axis=0) == 2)\n",
    "\n",
    "        green_chars = [guesses[x][y] for x,y in np.argwhere(history == 2) ]\n",
    "        orange_chars = [guesses[x][y] for x,y in np.argwhere(history == 1) ]\n",
    "        black_chars = [guesses[x][y] for x,y in np.argwhere(history == 0) ]\n",
    "        num_other_letters = len(set(orange_chars) - set(green_chars))\n",
    "        num_black_letters = len(set(black_chars))\n",
    "\n",
    "        sequence_number = int(history.size / 5)\n",
    "        #print(f'construct_state() with seqno {sequence_number}')\n",
    "\n",
    "        sequence_number_onehot = np.zeros(Env.num_guesses)\n",
    "        sequence_number_onehot[sequence_number] = 1.0\n",
    "        arr = np.concatenate((np.array([num_green_locs, num_other_letters, num_black_letters])/5, sequence_number_onehot))\n",
    "        return torch.tensor(arr, device=device, dtype=torch.float)\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0339f14",
   "metadata": {},
   "source": [
    "The aim here is to use a NN to represent the policy, rather than the value function.  We will shrink the action space (ie, so that we have a few actions, rather than 12000).  This will remove the model's ability to learn novel strategies, rather it will just be learning when to employ the different strategies (actions) that I give it.  Start w\n",
    "ith these 3 word selection tactics:\n",
    "\n",
    "1. choose words which match the current history\n",
    "1. choose words which contain the greatest number of new letters\n",
    "1. choose words which have the highest frequency score\n",
    "\n",
    "then we will construct 6 actions by choosing every possible order of these strategies\n",
    "1. 1,2,3\n",
    "1. 1,3,2\n",
    "1. 2,1,3\n",
    "1. 2,3,1\n",
    "1. 3,1,2\n",
    "1. 3,2,1\n",
    "\n",
    "for all these actions there may be multiple words, so sample a random one.  The policy then becomes a logistic regressor which selects one of these actions to execute.  The loss to train the regressor will be derived using the policy gradiet theorem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9dd99de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count, permutations\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "#plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "832e7980",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def clear(self):\n",
    "        self.memory.clear()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ae28e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetNN(nn.Module):\n",
    "\n",
    "    def __init__(self, num_inputs, num_actions):\n",
    "        super(PolicyNetNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_inputs, 20)\n",
    "        self.head = nn.Linear(20, num_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return F.softmax(self.head(x), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c642286",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetLinear(nn.Module):\n",
    "\n",
    "    def __init__(self, num_inputs, num_actions):\n",
    "        super(PolicyNetLinear, self).__init__()\n",
    "        self.head = nn.Linear(num_inputs, num_actions)\n",
    "        #print(f'PolicyNetLinear {num_inputs}, {num_actions}')\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        return F.softmax(self.head(x), dim=0)\n",
    "    \n",
    "class PolicyHybrid(nn.Module):\n",
    "    def __init__(self, num_guesses, num_inputs, num_actions):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.functions = torch.nn.ModuleList()\n",
    "        self.function0 = torch.nn.Parameter(torch.ones((num_actions), dtype=float), requires_grad=True)\n",
    "        for i in range(1, num_guesses):\n",
    "            self.functions.append(nn.Linear(num_inputs - num_guesses, num_actions))\n",
    "        self.x = torch.Tensor([1.0,1.0])\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, state):\n",
    "        onehot = state[3:]\n",
    "        step_idx = torch.argmax(onehot, dim=0)\n",
    "        if step_idx == 0:\n",
    "            y = self.x.mul(self.function0)\n",
    "        else:\n",
    "            x = state[0:3].to(device)\n",
    "            y = self.functions[step_idx-1](x)\n",
    "        return F.softmax(y, dim=0)\n",
    "    \n",
    "class PolicyMonteCarlo(nn.Module):\n",
    "    def __init__(self, num_guesses, num_actions):\n",
    "        super(PolicyMonteCarlo, self).__init__()\n",
    "        #self.weights = torch.nn.Parameter(torch.rand((num_guesses, num_actions), dtype=float), requires_grad=True)\n",
    "        self.weights = torch.nn.Parameter(torch.ones((num_guesses, num_actions), dtype=float), requires_grad=True)\n",
    "        #self.weights.require_grad = True\n",
    "        self.x = torch.Tensor([1.0,1.0])\n",
    "        \n",
    "    def forward(self, state):\n",
    "        onehot = state[3:]\n",
    "        step_idx = torch.argmax(onehot, dim=0)\n",
    "        \n",
    "        y = F.softmax(self.x.mul(self.weights[step_idx]), dim=0)\n",
    "        #print(f'onehot {onehot}, step_idx {step_idx}')\n",
    "        #print(self.weights)\n",
    "        #print(self.weights[step_idx])\n",
    "        #print(y)\n",
    "        return y\n",
    "    \n",
    "class PolicyAvgReward():\n",
    "    def __init__(self, num_guesses, num_actions):\n",
    "        #self.weights = torch.Tensor([[.5,.5],[1,0],[1,0],[1,0],[1,0],[1,0]])\n",
    "        self.weights = torch.rand((num_guesses, num_actions), dtype=float)\n",
    "        \n",
    "        self.reward_stats = [(defaultdict(int),defaultdict(int)) for _ in range(num_guesses)]\n",
    "        self.num_guesses = num_guesses\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "    def __call__(self, state):\n",
    "        onehot = state[3:]\n",
    "        step_idx = torch.argmax(onehot, dim=0)\n",
    "        #if step_idx == 0:\n",
    "        return F.softmax(self.weights[step_idx], dim=0)\n",
    "        #else:\n",
    "        #    return self.weights[step_idx]\n",
    "    \n",
    "    def calc_avgs(self):\n",
    "        for action in range(self.num_actions):\n",
    "            for step_idx in range(self.num_guesses):\n",
    "                if self.reward_stats[step_idx][action]['count'] > 0:\n",
    "                    self.weights[step_idx][action] = self.reward_stats[step_idx][action]['total'] / self.reward_stats[step_idx][action]['count']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84174e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the word-selection tactics\n",
    "n_state_features = 9\n",
    "\n",
    "class PolicyHelper:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.actions = [[env.find_target_words, env.find_words_matching_current_history]]\n",
    "        self.actions.append([env.find_words_with_highest_new_letter_freq_score])\n",
    "                \n",
    "        self.num_actions = len(self.actions)\n",
    "        #self.net = PolicyNetLinear(n_state_features, len(self.actions))\n",
    "        \n",
    "    def perform_action(self, action_idx):\n",
    "        tactic_tuple = self.actions[action_idx]\n",
    "        df = self.env.df\n",
    "        for tactic in tactic_tuple: # apply all the tactics in the given order\n",
    "            newdf = tactic(df)\n",
    "            if not newdf.empty: #if that tactic produced no results, then quit\n",
    "                df = newdf\n",
    "        return df.sample()['word'][0] # then pick a random word from what is left\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb9ed4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_values(vals, axes=['duration', 'episode']):\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel(axes[1])\n",
    "    plt.ylabel(axes[0])\n",
    "    plt.plot(np.array(vals))\n",
    "    # Take 20 episode averages and plot them too\n",
    "    window_width = 20\n",
    "    if len(vals) >= window_width:\n",
    "        cumsum_vec = np.cumsum(np.insert(vals, 0, 0)) \n",
    "        ma_vec = (cumsum_vec[window_width:] - cumsum_vec[:-window_width]) / window_width\n",
    "        plt.plot(np.insert(ma_vec, 0, [None]*int(window_width/2)))\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    #if is_ipython:\n",
    "    #    display.clear_output(wait=True)\n",
    "    #    display.display(plt.gcf())\n",
    "    \n",
    "def plot_all(episode_durations, episode_rewards, losses, epsilons, gammas):\n",
    "    plot_values(episode_durations, axes=['duration', 'episode'])\n",
    "    plot_values(episode_rewards, axes=['reward', 'episode'])\n",
    "    if losses: plot_values(losses, axes=['loss', 'step'])\n",
    "    if epsilons: plot_values(epsilons, axes=['epsilon', 'step'])\n",
    "    if gammas: plot_values(gammas, axes=['gamma', 'step'])\n",
    "    #plt.ioff()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14c1200d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model_batch(model, optimizer, memory, batch_size=128):\n",
    "    if batch_size <= 0:\n",
    "        transitions = memory.memory\n",
    "    else:\n",
    "        transitions = memory.sample(batch_size)\n",
    "    print(f'optimize_model_batch {batch_size} {len(transitions)}')\n",
    "    losses = [optimize_model_single(model, optimizer, tr.state, tr.action, tr.reward) for tr in transitions]\n",
    "    \n",
    "    return losses\n",
    "\n",
    "optimizations_run = 0\n",
    "\n",
    "def optimize_model_single(model, optimizer, state, action, reward):\n",
    "    global optimizations_run\n",
    "    optimizations_run += 1\n",
    "    if isinstance(model, PolicyAvgReward):\n",
    "        onehot = state[3:]\n",
    "        step_idx = torch.argmax(onehot, dim=0)\n",
    "        model.reward_stats[step_idx][action]['count'] += 1\n",
    "        model.reward_stats[step_idx][action]['total'] += reward\n",
    "        model.calc_avgs()\n",
    "        return reward\n",
    "    # calculate gradient\n",
    "    probs = model(state)\n",
    "    sampler = Categorical(probs)\n",
    "    #print(f'sampler {sampler}')\n",
    "    log_probs = -sampler.log_prob(action)   # \"-\" because it was built to work with gradient descent, but we are using gradient ascent\n",
    "\n",
    "    pseudo_loss = log_probs * reward # loss that when differentiated with autograd gives the gradient of J(θ)\n",
    "    #print(f'log_prob {log_probs}, reward {reward}, loss {pseudo_loss} ')\n",
    "    # update policy weights\n",
    "    optimizer.zero_grad()\n",
    "    pseudo_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return pseudo_loss\n",
    "\n",
    "\n",
    "class TrainConfig():\n",
    "    def __init__(self, optimizer='adam', batch_size=64, train_interval=64, clear_memory=False, lr=0.01):\n",
    "        self.optimizer = optimizer\n",
    "        self.clear_memory = clear_memory\n",
    "        self.lr = lr\n",
    "        self.train_interval = train_interval\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "class ValueConfig():\n",
    "    def __init__(self, name='reward', gamma=[0.9, 0.05, 200]):\n",
    "        self.name = name\n",
    "        self.gamma = gamma\n",
    "        \n",
    "class ModelConfig():\n",
    "    def __init__(self, name='naive', startword=None, target_list_only=None):\n",
    "        self.name = name\n",
    "        self.startword = startword\n",
    "        self.target_list_only = target_list_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71c62073",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "def run_experiment(model=ModelConfig(name='naive', startword=None, target_list_only=False),\n",
    "                   num_episodes=128,\n",
    "                   eps=[0.9, 0.05, 200],\n",
    "                   value_function=ValueConfig(name='reward',gamma=[0.0, 1.0, 200]),\n",
    "                   training=TrainConfig(),\n",
    "                   seed=0,\n",
    "                   run_test=False):\n",
    "    global optimizations_run\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    GAMMA_START, GAMMA_END, GAMMA_DECAY = value_function.gamma\n",
    "    env = Env(df)\n",
    "    memory = ReplayMemory(10000)\n",
    "    starting_state = construct_state_tensor(env.guesses, env.history)\n",
    "\n",
    "    steps_done = 0\n",
    "    last_training = 0\n",
    "    losses = []\n",
    "    episode_rewards = []\n",
    "    episode_durations = []\n",
    "    epsilons = []\n",
    "    gammas = []\n",
    "    reward_stats = [(defaultdict(int),defaultdict(int)) for _ in range(env.num_guesses)]\n",
    "    transitions_added_to_memory = 0\n",
    "    optimizations_run = 0\n",
    "    \n",
    "    policy_helper = PolicyHelper(env)\n",
    "    \n",
    "    if model.name == 'linear':\n",
    "        policy_net = PolicyNetLinear(n_state_features, len(policy_helper.actions)).to(device)\n",
    "    elif model.name == 'monte':\n",
    "        policy_net = PolicyMonteCarlo(env.num_guesses, len(policy_helper.actions)).to(device)\n",
    "        print('monte weights')\n",
    "        print(policy_net.weights)\n",
    "        print(F.softmax(policy_net.weights, dim=1))\n",
    "    elif model.name == 'avg_reward':\n",
    "        policy_net = PolicyAvgReward(env.num_guesses, len(policy_helper.actions))\n",
    "    elif model.name == 'hybrid':\n",
    "        policy_net = PolicyHybrid(env.num_guesses, n_state_features, len(policy_helper.actions))\n",
    "    else:\n",
    "        policy_net = PolicyNetNN(n_state_features, len(policy_helper.actions)).to(device)\n",
    "    \n",
    "    if model.name == 'avg_reward':\n",
    "        optimizer = None\n",
    "    elif training.optimizer == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(policy_net.parameters(), lr=training.lr)\n",
    "    elif training.optimizer == 'sgd':\n",
    "        optimizer = optim.SGD(policy_net.parameters(), lr=training.lr)\n",
    "    else:\n",
    "        optimizer = optim.Adam(policy_net.parameters(), lr=training.lr)\n",
    "\n",
    "        #print(f'pn params {list(policy_net.parameters())}')\n",
    "    for i_episode in range(num_episodes):\n",
    "        # Initialize the environment and state\n",
    "        env.reset()\n",
    "        print(f'=========================episode {i_episode} {env.target}======================')\n",
    "\n",
    "        episode_memory = []\n",
    "        state = starting_state\n",
    "        guesses = []\n",
    "        for t in count():\n",
    "            GAMMA = GAMMA_END + (GAMMA_START - GAMMA_END) * math.exp(-1. * steps_done / GAMMA_DECAY)\n",
    "            gammas.append(GAMMA)\n",
    "            steps_done += 1\n",
    "            # Select and perform an action\n",
    "            #print(state, actions)\n",
    "            probs = policy_net(state)\n",
    "            sampler = Categorical(probs)\n",
    "            action_idx = sampler.sample()\n",
    "            chosen_word = policy_helper.perform_action(action_idx)\n",
    "            guesses.append(chosen_word)\n",
    "            print(f'------guess {t} {action_idx} {guesses[-1]}-------')\n",
    "            history, reward, done = env.step(chosen_word)\n",
    "            #here next_state == env.history\n",
    "            if not done:\n",
    "                next_state = construct_state_tensor(guesses, history)\n",
    "            \n",
    "            #action_tensor = action_idx.clone().detach()\n",
    "            action = action_idx #F.one_hot(action_idx, num_classes=policy_helper.num_actions)\n",
    "                \n",
    "            print(f'reward {reward} done {done} action {action}')\n",
    "            #reward = np.array([reward])\n",
    "\n",
    "            # Store the transition in memory\n",
    "            #memory.push(state, action_idx, reward)\n",
    "            episode_memory.append([state, action, reward])\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                episode_durations.append(t + 1)\n",
    "                episode_reward = sum([tr[2] for tr in episode_memory])\n",
    "                print(f'episode {i_episode} finished.  reward {episode_reward}  eps {eps}  gamma {GAMMA}  steps {steps_done}  memory {len(memory)}')\n",
    "                episode_rewards.append(episode_reward)\n",
    "                \n",
    "                for idx,tr in enumerate(episode_memory):\n",
    "                    #print(f'pushing episode_reward {episode_reward} {episode_reward.__class__}')\n",
    "                    transitions_added_to_memory += 1\n",
    "                    memory.push(tr[0], tr[1], episode_reward)\n",
    "                    #loss = optimize_model_single(policy_net, optimizer, tr[0], tr[1], episode_reward)\n",
    "                    #losses.append(loss)\n",
    "                \n",
    "                    reward_stats[idx][tr[1]]['count'] += 1\n",
    "                    reward_stats[idx][tr[1]]['total'] += episode_reward\n",
    "                    episode_reward -= tr[2]\n",
    "                    \n",
    "                    \n",
    "                # If we have gathered enough data, Perform one step of the optimization (on the policy network)\n",
    "                if len(memory) >= max(1,training.batch_size) \\\n",
    "                    and (i_episode + 1) % training.train_interval == 0:\n",
    "                    losses += optimize_model_batch(policy_net, optimizer, memory, batch_size=training.batch_size)\n",
    "                    if training.clear_memory: memory.clear()\n",
    "                        \n",
    "                    if model.name == 'monte':\n",
    "                        print('monte weights')\n",
    "                        print(policy_net.weights)\n",
    "                        print(F.softmax(policy_net.weights, dim=1))\n",
    "                        for rs in reward_stats:\n",
    "                            if rs[0]['count'] > 0:\n",
    "                                rs[0]['avg'] = rs[0]['total'] / rs[0]['count']\n",
    "                            if rs[1]['count'] > 0:\n",
    "                                rs[1]['avg'] = rs[1]['total'] / rs[1]['count']\n",
    "                            print(dict(rs[0]), dict(rs[1]))\n",
    "                    elif model.name == 'avg_reward':\n",
    "                        print('avg_reward weights')\n",
    "                        print(policy_net.weights)\n",
    "                        print(F.softmax(policy_net.weights, dim=1))\n",
    "                        for rs in policy_net.reward_stats:\n",
    "                            if rs[0]['count'] > 0:\n",
    "                                rs[0]['avg'] = rs[0]['total'] / rs[0]['count']\n",
    "                            if rs[1]['count'] > 0:\n",
    "                                rs[1]['avg'] = rs[1]['total'] / rs[1]['count']\n",
    "                            print(dict(rs[0]), dict(rs[1]))\n",
    "                    elif model.name == 'hybrid':\n",
    "                        print('hybrid weights')\n",
    "                        print(policy_net.function0)\n",
    "                        print(F.softmax(policy_net.function0, dim=0))\n",
    "                    print(f'done {optimizations_run} optimizations, {transitions_added_to_memory} transitions added to memory')\n",
    "                    \n",
    "                        \n",
    "                        \n",
    "                \n",
    "                #plot_durations()\n",
    "                break\n",
    "\n",
    "    print('Training Complete')\n",
    "    \n",
    "    if run_test:\n",
    "        performance_hist = [0] * (1 + env.num_guesses)\n",
    "        for e in env.foreach_target_word():\n",
    "            state = starting_state\n",
    "            done = False\n",
    "            reward = 0\n",
    "            guesses = []\n",
    "            while not done:\n",
    "                probs = policy_net(state)\n",
    "                sampler = Categorical(probs)\n",
    "                action_idx = sampler.sample()\n",
    "                chosen_word = policy_helper.perform_action(action_idx)\n",
    "                guesses.append(chosen_word)\n",
    "                _, reward, done = e.step(chosen_word)\n",
    "                if not done:\n",
    "                    state = construct_state_tensor(guesses, history)\n",
    "            \n",
    "            num_guesses = len(guesses)\n",
    "            if num_guesses == 6 and reward == -1:\n",
    "                num_guesses = 0\n",
    "            print(f'{e.target} {num_guesses}')    \n",
    "            performance_hist[num_guesses] += 1\n",
    "            \n",
    "        for i,p in enumerate(performance_hist):\n",
    "            print(f'{i}: {p}')\n",
    "    \n",
    "    return episode_durations, episode_rewards, losses, epsilons, gammas\n",
    "\n",
    "#env.render()\n",
    "#env.close()\n",
    "#plt.ioff()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2b5e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100a9909",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all(*run_experiment(\n",
    "    model=ModelConfig(name='avg_reward'),\n",
    "    num_episodes=512,\n",
    "    training=TrainConfig(optimizer='sgd', lr=0.01, batch_size=-1, train_interval=128, clear_memory=True),\n",
    "    seed=1,\n",
    "    run_test=True\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bbee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all(*run_experiment(\n",
    "    model=ModelConfig(name='monte'),\n",
    "    num_episodes=512,\n",
    "    training=TrainConfig(optimizer='adam', lr=0.01, batch_size=-1, train_interval=128, clear_memory=True),\n",
    "    seed=1,\n",
    "    run_test=True\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1b6a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all(*run_experiment(\n",
    "    model=ModelConfig(name='linear'),\n",
    "    num_episodes=512,\n",
    "    training=TrainConfig(optimizer='rmprop', lr=0.01, batch_size=-1, train_interval=128, clear_memory=True),\n",
    "    seed=1,\n",
    "    run_test=True\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64f6e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all(*run_experiment(\n",
    "    model=ModelConfig(name='nn'),\n",
    "    num_episodes=512,\n",
    "    training=TrainConfig(optimizer='rmsprop', lr=0.01, batch_size=-1, train_interval=128, clear_memory=True),\n",
    "    seed=1,\n",
    "    run_test=True\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f21632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all(*run_experiment(\n",
    "    model=ModelConfig(name='hybrid'),\n",
    "    num_episodes=256,\n",
    "    training=TrainConfig(optimizer='sgd', lr=0.01, batch_size=-1, train_interval=128, clear_memory=True),\n",
    "    seed=1,\n",
    "    run_test=True\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0797f8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all(*run_experiment(\n",
    "    model=ModelConfig(name='hybrid'),\n",
    "    num_episodes=384,\n",
    "    training=TrainConfig(optimizer='adam', lr=0.01, batch_size=-1, train_interval=128, clear_memory=True),\n",
    "    seed=1,\n",
    "    run_test=True\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80353f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all(*run_experiment(\n",
    "    model=ModelConfig(name='hybrid'),\n",
    "    num_episodes=511,\n",
    "    training=TrainConfig(optimizer='rmsprop', lr=0.01, batch_size=-1, train_interval=128, clear_memory=True),\n",
    "    seed=1,\n",
    "    run_test=True\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8ece0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
