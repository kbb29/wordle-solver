{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8ebc590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, pathlib, random, time\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "\n",
    "from environment import Env, validate_against_hint, load_word_lists, construct_word_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae1b8e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = construct_word_df(*load_word_lists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "895b599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def construct_state_tensor(guesses, history):\n",
    "        #print(history)\n",
    "        #so the state is going to be:\n",
    "            #  The number of green locations we know\n",
    "            #  The number of other letters we know to be in the word\n",
    "            #  The sequence number of the guess (1st guess, 2nd guess etc.)\n",
    "\n",
    "        #the number of locations which were green at some point in the history\n",
    "        num_green_locs = np.count_nonzero(history.max(axis=0) == 2)\n",
    "\n",
    "        green_chars = [guesses[x][y] for x,y in np.argwhere(history == 2) ]\n",
    "        orange_chars = [guesses[x][y] for x,y in np.argwhere(history == 1) ]\n",
    "        black_chars = [guesses[x][y] for x,y in np.argwhere(history == 0) ]\n",
    "        num_other_letters = len(set(orange_chars) - set(green_chars))\n",
    "        num_black_letters = len(set(black_chars))\n",
    "\n",
    "        sequence_number = int(history.size / 5)\n",
    "        #print(f'construct_state() with seqno {sequence_number}')\n",
    "\n",
    "        sequence_number_onehot = np.zeros(Env.num_guesses)\n",
    "        sequence_number_onehot[sequence_number] = 1.0\n",
    "        arr = np.concatenate((np.array([num_green_locs, num_other_letters, num_black_letters])/5, sequence_number_onehot))\n",
    "        return torch.tensor(arr, device=device, dtype=torch.float)\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0339f14",
   "metadata": {},
   "source": [
    "The aim here is to use a NN to represent the policy, rather than the value function.  We will shrink the action space (ie, so that we have a few actions, rather than 12000).  This will remove the model's ability to learn novel strategies, rather it will just be learning when to employ the different strategies (actions) that I give it.  Start w\n",
    "ith these 3 word selection tactics:\n",
    "\n",
    "1. choose words which match the current history\n",
    "1. choose words which contain the greatest number of new letters\n",
    "1. choose words which have the highest frequency score\n",
    "\n",
    "then we will construct 6 actions by choosing every possible order of these strategies\n",
    "1. 1,2,3\n",
    "1. 1,3,2\n",
    "1. 2,1,3\n",
    "1. 2,3,1\n",
    "1. 3,1,2\n",
    "1. 3,2,1\n",
    "\n",
    "for all these actions there may be multiple words, so sample a random one.  The policy then becomes a logistic regressor which selects one of these actions to execute.  The loss to train the regressor will be derived using the policy gradiet theorem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9dd99de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count, permutations\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "#plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "832e7980",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def clear(self):\n",
    "        self.memory.clear()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ae28e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetNN(nn.Module):\n",
    "\n",
    "    def __init__(self, num_inputs, num_actions):\n",
    "        super(PolicyNetNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_inputs, 20)\n",
    "        self.head = nn.Linear(20, num_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return F.softmax(self.head(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c642286",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetLinear(nn.Module):\n",
    "\n",
    "    def __init__(self, num_inputs, num_actions):\n",
    "        super(PolicyNetLinear, self).__init__()\n",
    "        self.head = nn.Linear(num_inputs, num_actions)\n",
    "        #print(f'PolicyNetLinear {num_inputs}, {num_actions}')\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        print(f'forward() {x.shape}')\n",
    "        x = x.to(device)\n",
    "        return F.softmax(self.head(x), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59ce0072",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the word-selection tactics\n",
    "n_state_features = 9\n",
    "\n",
    "class PolicyHelper:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        actions = [[env.find_words_matching_current_history]]\n",
    "        actions.append([env.find_words_matching_current_history, env.find_words_with_highest_freq_score])\n",
    "        actions.append([env.find_words_matching_current_history, env.find_words_with_most_new_letters])\n",
    "        actions.append([env.find_words_matching_current_history, env.find_words_with_most_new_letters, env.find_words_with_highest_freq_score])\n",
    "        actions.append([env.find_words_with_most_new_letters, env.find_words_with_highest_freq_score])\n",
    "        actions.append([env.find_words_with_most_new_letters])\n",
    "        \n",
    "        self.actions = actions.copy()\n",
    "        for a in actions:\n",
    "                self.actions.append([env.find_target_words] + a)\n",
    "                \n",
    "        self.num_actions = len(self.actions)\n",
    "        #self.net = PolicyNetLinear(n_state_features, len(self.actions))\n",
    "        \n",
    "    def perform_action(self, action_idx):\n",
    "        tactic_tuple = self.actions[action_idx]\n",
    "        df = self.env.df\n",
    "        for tactic in tactic_tuple: # apply all the tactics in the given order\n",
    "            newdf = tactic(df)\n",
    "            if not newdf.empty: #if that tactic produced no results, then quit\n",
    "                df = newdf\n",
    "        return df.sample()['word'][0] # then pick a random word from what is left\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb9ed4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_values(vals, axes=['duration', 'episode']):\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel(axes[1])\n",
    "    plt.ylabel(axes[0])\n",
    "    plt.plot(np.array(vals))\n",
    "    # Take 20 episode averages and plot them too\n",
    "    window_width = 20\n",
    "    if len(vals) >= window_width:\n",
    "        cumsum_vec = np.cumsum(np.insert(vals, 0, 0)) \n",
    "        ma_vec = (cumsum_vec[window_width:] - cumsum_vec[:-window_width]) / window_width\n",
    "        plt.plot(np.insert(ma_vec, 0, [None]*int(window_width/2)))\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    #if is_ipython:\n",
    "    #    display.clear_output(wait=True)\n",
    "    #    display.display(plt.gcf())\n",
    "    \n",
    "def plot_all(episode_durations, episode_rewards, losses, epsilons, gammas):\n",
    "    plot_values(episode_durations, axes=['duration', 'episode'])\n",
    "    plot_values(episode_rewards, axes=['reward', 'episode'])\n",
    "    if losses: plot_values(losses, axes=['loss', 'step'])\n",
    "    if epsilons: plot_values(epsilons, axes=['epsilon', 'step'])\n",
    "    if gammas: plot_values(gammas, axes=['gamma', 'step'])\n",
    "    #plt.ioff()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "14c1200d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(model, optimizer, memory, batch_size=128):\n",
    "    transitions = memory.sample(batch_size)\n",
    "\n",
    "    state_batch = torch.stack([tr.state for tr in transitions])\n",
    "    action_batch = torch.stack([tr.action for tr in transitions])\n",
    "    reward_batch = np.stack([tr.reward for tr in transitions])\n",
    "    \n",
    "    \n",
    "    # calculate gradient\n",
    "    probs = model(state_batch)\n",
    "    sampler = Categorical(probs)\n",
    "    print(f'optimize_model() {action_batch.shape}\\n{reward_batch}')\n",
    "    log_probs = -sampler.log_prob(action_batch)   # \"-\" because it was built to work with gradient descent, but we are using gradient ascent\n",
    "    \n",
    "    print(f'optimize_model() {log_probs}')\n",
    "    pseudo_loss = torch.sum(log_probs * reward_batch) # loss that when differentiated with autograd gives the gradient of J(θ)\n",
    "    # update policy weights\n",
    "    optimizer.zero_grad()\n",
    "    pseudo_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return pseudo_loss\n",
    "\n",
    "class TrainConfig():\n",
    "    def __init__(self, train_interval=128, batch_size=128, clear_memory=False, lr=0.01):\n",
    "        self.train_interval = train_interval\n",
    "        self.batch_size = batch_size\n",
    "        self.clear_memory = clear_memory\n",
    "        self.lr = lr\n",
    "        \n",
    "class ValueConfig():\n",
    "    def __init__(self, name='reward', gamma=[0.9, 0.05, 200]):\n",
    "        self.name = name\n",
    "        self.gamma = gamma\n",
    "        \n",
    "class ModelConfig():\n",
    "    def __init__(self, name='naive', startword=None, target_list_only=None):\n",
    "        self.name = name\n",
    "        self.startword = startword\n",
    "        self.target_list_only = target_list_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "71c62073",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "def run_experiment(model=ModelConfig(name='naive', startword=None, target_list_only=False),\n",
    "                   num_episodes=128,\n",
    "                   eps=[0.9, 0.05, 200],\n",
    "                   value_function=ValueConfig(name='reward',gamma=[0.0, 1.0, 200]),\n",
    "                   training=TrainConfig(clear_memory=False, batch_size=128, train_interval=128)):\n",
    "    torch.manual_seed(0)\n",
    "    random.seed(0)\n",
    "    np.random.seed(0)\n",
    "    GAMMA_START, GAMMA_END, GAMMA_DECAY = value_function.gamma\n",
    "    env = Env(df)\n",
    "    memory = ReplayMemory(10000)\n",
    "    print('run_experiment()')\n",
    "    starting_state = construct_state_tensor(env.guesses, env.history)\n",
    "\n",
    "    print('constructed state tensor')\n",
    "    steps_done = 0\n",
    "    last_training = 0\n",
    "    losses = []\n",
    "    episode_rewards = []\n",
    "    episode_durations = []\n",
    "    epsilons = []\n",
    "    gammas = []\n",
    "    \n",
    "    policy_helper = PolicyHelper(env)\n",
    "    \n",
    "    print('made policy helper')\n",
    "    \n",
    "    if model.name == 'linear':\n",
    "        policy_net = PolicyNetLinear(n_state_features, len(policy_helper.actions)).to(device)\n",
    "        optimizer = optim.Adam(policy_net.parameters(), lr=training.lr)\n",
    "    else:\n",
    "        policy_net = PolicyNetNN(n_state_features, len(policy_helper.actions)).to(device)\n",
    "        optimizer = optim.Adam(policy_net.parameters(), lr=training.lr)\n",
    "\n",
    "    print(f'pn params {list(policy_net.parameters())}')\n",
    "    for i_episode in range(num_episodes):\n",
    "        # Initialize the environment and state\n",
    "        env.reset()\n",
    "        print(f'=========================episode {i_episode} {env.target}======================')\n",
    "\n",
    "        episode_memory = []\n",
    "        state = starting_state\n",
    "        guesses = []\n",
    "        for t in count():\n",
    "            GAMMA = GAMMA_END + (GAMMA_START - GAMMA_END) * math.exp(-1. * steps_done / GAMMA_DECAY)\n",
    "            gammas.append(GAMMA)\n",
    "            steps_done += 1\n",
    "            # Select and perform an action\n",
    "            #print(state, actions)\n",
    "            probs = policy_net(state)\n",
    "            sampler = Categorical(probs)\n",
    "            action_idx = sampler.sample()\n",
    "            chosen_word = policy_helper.perform_action(action_idx)\n",
    "            guesses.append(chosen_word)\n",
    "            print(f'------guess {t} {action_idx} {guesses[-1]}-------')\n",
    "            history, reward, done = env.step(chosen_word)\n",
    "            #here next_state == env.history\n",
    "            if not done:\n",
    "                next_state = construct_state_tensor(guesses, history)\n",
    "            \n",
    "            #action_tensor = action_idx.clone().detach()\n",
    "            action = F.one_hot(action_idx, num_classes=policy_helper.num_actions)\n",
    "                \n",
    "            print(f'reward {reward} done {done} action shape {action_idx.__class__} {action.shape} {action}')\n",
    "            #reward = np.array([reward])\n",
    "\n",
    "            # Store the transition in memory\n",
    "            #memory.push(state, action_idx, reward)\n",
    "            episode_memory.append([state, action, reward])\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                episode_durations.append(t + 1)\n",
    "                episode_reward = sum([tr[2] for tr in episode_memory])\n",
    "                print(f'episode {i_episode} finished.  reward {episode_reward}  eps {eps}  gamma {GAMMA}  steps {steps_done}  memory {len(memory)}')\n",
    "                episode_rewards.append(episode_reward)\n",
    "                \n",
    "                for tr in episode_memory:\n",
    "                    print(f'pushing episode_reward {episode_reward} {episode_reward.__class__}')\n",
    "                    memory.push(tr[0], tr[1], episode_reward)\n",
    "                    episode_reward -= tr[2]\n",
    "                \n",
    "                # If we have gathered enough data, Perform one step of the optimization (on the policy network)\n",
    "                if len(memory) >= training.batch_size \\\n",
    "                    and steps_done - last_training > training.train_interval:\n",
    "                    loss = optimize_model(policy_net, optimizer, memory, batch_size=training.batch_size)\n",
    "                    losses.append(loss)\n",
    "                    if training.clear_memory: memory.clear()\n",
    "                    last_training = steps_done\n",
    "                #plot_durations()\n",
    "                break\n",
    "\n",
    "    print('Complete')\n",
    "    \n",
    "    return episode_durations, episode_rewards, losses, epsilons, gammas\n",
    "\n",
    "#env.render()\n",
    "#env.close()\n",
    "#plt.ioff()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f94916dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_experiment()\n",
      "constructed state tensor\n",
      "made policy helper\n",
      "pn params [Parameter containing:\n",
      "tensor([[-0.0025,  0.1788, -0.2743, -0.2453, -0.1284,  0.0894, -0.0066,  0.2643,\n",
      "         -0.0296],\n",
      "        [ 0.0882, -0.1007, -0.0655, -0.3184, -0.2208, -0.1374,  0.0123,  0.1318,\n",
      "          0.2000],\n",
      "        [-0.2260, -0.1452,  0.1211,  0.2768, -0.0686,  0.2494, -0.0537,  0.0353,\n",
      "          0.3018],\n",
      "        [-0.3092, -0.2098, -0.0844, -0.1299,  0.2880, -0.2161, -0.1534, -0.2329,\n",
      "         -0.3122],\n",
      "        [-0.1946,  0.2865,  0.1487,  0.1616,  0.0175, -0.1709,  0.0564, -0.3112,\n",
      "         -0.2409],\n",
      "        [-0.1718,  0.2103,  0.1954, -0.1478, -0.0120,  0.2132,  0.3314,  0.1323,\n",
      "          0.0450],\n",
      "        [ 0.2235, -0.1963,  0.0621, -0.2584, -0.2310, -0.1722,  0.1508,  0.1341,\n",
      "         -0.1975],\n",
      "        [ 0.1007,  0.1830, -0.0421,  0.0127,  0.0772,  0.2068,  0.3201, -0.2569,\n",
      "         -0.1222],\n",
      "        [ 0.1310,  0.2762,  0.2901,  0.2941,  0.0663, -0.2899,  0.0307, -0.2085,\n",
      "         -0.3107],\n",
      "        [ 0.2962,  0.2535, -0.3325,  0.0624, -0.0562, -0.0549, -0.1526,  0.1282,\n",
      "         -0.1974],\n",
      "        [ 0.1222,  0.1686,  0.2386,  0.1246, -0.3299, -0.2162,  0.1664,  0.0698,\n",
      "         -0.2600],\n",
      "        [-0.1919,  0.3136,  0.2246, -0.1453, -0.0839, -0.3175, -0.0060, -0.2510,\n",
      "         -0.2571]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.0184,  0.0500, -0.1365,  0.1978, -0.2028,  0.3025,  0.2284, -0.2811,\n",
      "        -0.0830,  0.0150,  0.0486,  0.0791], requires_grad=True)]\n",
      "=========================episode 0 lorry======================\n",
      "forward() torch.Size([9])\n",
      "------guess 0 7 erase-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 1 0 flurr-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 2 2 lorry-------\n",
      "reward 0.0 done True action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "episode 0 finished.  reward -2.0  eps [0.0, 0.0, 400]  gamma 0.0  steps 3  memory 0\n",
      "pushing episode_reward -2.0 <class 'float'>\n",
      "pushing episode_reward -1.0 <class 'float'>\n",
      "pushing episode_reward 0.0 <class 'float'>\n",
      "=========================episode 1 phony======================\n",
      "forward() torch.Size([9])\n",
      "------guess 0 11 chirp-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
      "forward() torch.Size([9])\n",
      "------guess 1 7 shape-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 2 2 phony-------\n",
      "reward 0.0 done True action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "episode 1 finished.  reward -2.0  eps [0.0, 0.0, 400]  gamma 0.0  steps 6  memory 3\n",
      "pushing episode_reward -2.0 <class 'float'>\n",
      "pushing episode_reward -1.0 <class 'float'>\n",
      "pushing episode_reward 0.0 <class 'float'>\n",
      "=========================episode 2 stage======================\n",
      "forward() torch.Size([9])\n",
      "------guess 0 5 bleak-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 1 10 snort-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 2 1 sates-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 3 10 humid-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 4 10 cagey-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 5 10 sweep-------\n",
      "reward -1.0 done True action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])\n",
      "episode 2 finished.  reward -6.0  eps [0.0, 0.0, 400]  gamma 0.0  steps 12  memory 6\n",
      "pushing episode_reward -6.0 <class 'float'>\n",
      "pushing episode_reward -5.0 <class 'float'>\n",
      "pushing episode_reward -4.0 <class 'float'>\n",
      "pushing episode_reward -3.0 <class 'float'>\n",
      "pushing episode_reward -2.0 <class 'float'>\n",
      "pushing episode_reward -1.0 <class 'float'>\n",
      "=========================episode 3 amend======================\n",
      "forward() torch.Size([9])\n",
      "------guess 0 5 sound-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 1 8 grind-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 2 0 eland-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 3 4 tymps-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 4 3 amend-------\n",
      "reward 0.0 done True action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "episode 3 finished.  reward -4.0  eps [0.0, 0.0, 400]  gamma 0.0  steps 17  memory 12\n",
      "pushing episode_reward -4.0 <class 'float'>\n",
      "pushing episode_reward -3.0 <class 'float'>\n",
      "pushing episode_reward -2.0 <class 'float'>\n",
      "pushing episode_reward -1.0 <class 'float'>\n",
      "pushing episode_reward 0.0 <class 'float'>\n",
      "=========================episode 4 canoe======================\n",
      "forward() torch.Size([9])\n",
      "------guess 0 2 eldin-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 1 5 kagus-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 2 2 caner-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 3 4 mothy-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 4 5 bwazi-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 5 5 vaper-------\n",
      "reward -1.0 done True action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])\n",
      "episode 4 finished.  reward -6.0  eps [0.0, 0.0, 400]  gamma 0.0  steps 23  memory 17\n",
      "pushing episode_reward -6.0 <class 'float'>\n",
      "pushing episode_reward -5.0 <class 'float'>\n",
      "pushing episode_reward -4.0 <class 'float'>\n",
      "pushing episode_reward -3.0 <class 'float'>\n",
      "pushing episode_reward -2.0 <class 'float'>\n",
      "pushing episode_reward -1.0 <class 'float'>\n",
      "=========================episode 5 lupus======================\n",
      "forward() torch.Size([9])\n",
      "------guess 0 2 decoy-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 1 5 walks-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 2 6 bliss-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 3 8 lupus-------\n",
      "reward 0.0 done True action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])\n",
      "episode 5 finished.  reward -3.0  eps [0.0, 0.0, 400]  gamma 0.0  steps 27  memory 23\n",
      "pushing episode_reward -3.0 <class 'float'>\n",
      "pushing episode_reward -2.0 <class 'float'>\n",
      "pushing episode_reward -1.0 <class 'float'>\n",
      "pushing episode_reward 0.0 <class 'float'>\n",
      "=========================episode 6 scram======================\n",
      "forward() torch.Size([9])\n",
      "------guess 0 4 aeros-------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 1 6 strap-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 2 2 syrah-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 3 5 clung-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 4 10 media-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 5 11 borax-------\n",
      "reward -1.0 done True action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
      "episode 6 finished.  reward -6.0  eps [0.0, 0.0, 400]  gamma 0.0  steps 33  memory 27\n",
      "pushing episode_reward -6.0 <class 'float'>\n",
      "pushing episode_reward -5.0 <class 'float'>\n",
      "pushing episode_reward -4.0 <class 'float'>\n",
      "pushing episode_reward -3.0 <class 'float'>\n",
      "pushing episode_reward -2.0 <class 'float'>\n",
      "pushing episode_reward -1.0 <class 'float'>\n",
      "=========================episode 7 aside======================\n",
      "forward() torch.Size([9])\n",
      "------guess 0 0 board-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 1 8 daunt-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 2 2 ideal-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 3 3 aside-------\n",
      "reward 0.0 done True action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "episode 7 finished.  reward -3.0  eps [0.0, 0.0, 400]  gamma 0.0  steps 37  memory 33\n",
      "pushing episode_reward -3.0 <class 'float'>\n",
      "pushing episode_reward -2.0 <class 'float'>\n",
      "pushing episode_reward -1.0 <class 'float'>\n",
      "pushing episode_reward 0.0 <class 'float'>\n",
      "=========================episode 8 robot======================\n",
      "forward() torch.Size([9])\n",
      "------guess 0 2 manos-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 1 8 throb-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 2 7 robot-------\n",
      "reward 0.0 done True action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
      "episode 8 finished.  reward -2.0  eps [0.0, 0.0, 400]  gamma 0.0  steps 40  memory 37\n",
      "pushing episode_reward -2.0 <class 'float'>\n",
      "pushing episode_reward -1.0 <class 'float'>\n",
      "pushing episode_reward 0.0 <class 'float'>\n",
      "=========================episode 9 revue======================\n",
      "forward() torch.Size([9])\n",
      "------guess 0 2 yelps-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 1 11 might-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
      "forward() torch.Size([9])\n",
      "------guess 2 7 refer-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 3 7 revue-------\n",
      "reward 0.0 done True action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
      "episode 9 finished.  reward -3.0  eps [0.0, 0.0, 400]  gamma 0.0  steps 44  memory 40\n",
      "pushing episode_reward -3.0 <class 'float'>\n",
      "pushing episode_reward -2.0 <class 'float'>\n",
      "pushing episode_reward -1.0 <class 'float'>\n",
      "pushing episode_reward 0.0 <class 'float'>\n",
      "=========================episode 10 pouch======================\n",
      "forward() torch.Size([9])\n",
      "------guess 0 5 fyked-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 1 11 clang-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
      "forward() torch.Size([9])\n",
      "------guess 2 5 broth-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 3 11 swami-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
      "forward() torch.Size([9])\n",
      "------guess 4 10 equip-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 5 4 vexes-------\n",
      "reward -1.0 done True action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "episode 10 finished.  reward -6.0  eps [0.0, 0.0, 400]  gamma 0.0  steps 50  memory 44\n",
      "pushing episode_reward -6.0 <class 'float'>\n",
      "pushing episode_reward -5.0 <class 'float'>\n",
      "pushing episode_reward -4.0 <class 'float'>\n",
      "pushing episode_reward -3.0 <class 'float'>\n",
      "pushing episode_reward -2.0 <class 'float'>\n",
      "pushing episode_reward -1.0 <class 'float'>\n",
      "=========================episode 11 lusty======================\n",
      "forward() torch.Size([9])\n",
      "------guess 0 8 debit-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 1 5 vagus-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 2 6 stunk-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 3 3 rusty-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 4 8 musty-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 5 1 lusty-------\n",
      "reward 0.0 done True action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "episode 11 finished.  reward -5.0  eps [0.0, 0.0, 400]  gamma 0.0  steps 56  memory 50\n",
      "pushing episode_reward -5.0 <class 'float'>\n",
      "pushing episode_reward -4.0 <class 'float'>\n",
      "pushing episode_reward -3.0 <class 'float'>\n",
      "pushing episode_reward -2.0 <class 'float'>\n",
      "pushing episode_reward -1.0 <class 'float'>\n",
      "pushing episode_reward 0.0 <class 'float'>\n",
      "=========================episode 12 taboo======================\n",
      "forward() torch.Size([9])\n",
      "------guess 0 6 close-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 1 5 khazi-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 2 9 rayon-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 3 5 guimp-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 4 5 wadts-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 5 8 taboo-------\n",
      "reward 0.0 done True action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])\n",
      "episode 12 finished.  reward -5.0  eps [0.0, 0.0, 400]  gamma 0.0  steps 62  memory 56\n",
      "pushing episode_reward -5.0 <class 'float'>\n",
      "pushing episode_reward -4.0 <class 'float'>\n",
      "pushing episode_reward -3.0 <class 'float'>\n",
      "pushing episode_reward -2.0 <class 'float'>\n",
      "pushing episode_reward -1.0 <class 'float'>\n",
      "pushing episode_reward 0.0 <class 'float'>\n",
      "=========================episode 13 twine======================\n",
      "forward() torch.Size([9])\n",
      "------guess 0 0 freer-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "forward() torch.Size([9])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------guess 1 10 salon-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 2 0 inbye-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 3 11 dutch-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
      "forward() torch.Size([9])\n",
      "------guess 4 5 swamp-------\n",
      "reward -1.0 done False action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])\n",
      "forward() torch.Size([9])\n",
      "------guess 5 8 twine-------\n",
      "reward 0.0 done True action shape <class 'torch.Tensor'> torch.Size([12]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])\n",
      "episode 13 finished.  reward -5.0  eps [0.0, 0.0, 400]  gamma 0.0  steps 68  memory 62\n",
      "pushing episode_reward -5.0 <class 'float'>\n",
      "pushing episode_reward -4.0 <class 'float'>\n",
      "pushing episode_reward -3.0 <class 'float'>\n",
      "pushing episode_reward -2.0 <class 'float'>\n",
      "pushing episode_reward -1.0 <class 'float'>\n",
      "pushing episode_reward 0.0 <class 'float'>\n",
      "forward() torch.Size([64, 9])\n",
      "optimize_model() torch.Size([64, 12])\n",
      "[-1. -2.  0. -3. -5. -1. -3. -5. -1. -4. -3. -1. -2. -3. -1. -4. -5. -3.\n",
      " -2. -6.  0.  0. -2. -1. -3.  0.  0. -1. -2. -4. -1. -2. -5. -6. -3. -1.\n",
      " -6. -2. -5. -4. -1. -1. -4. -6. -2.  0. -4. -3. -3. -1. -2. -5. -2. -1.\n",
      " -2.  0. -2.  0. -2. -4.  0.  0. -3. -4.]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Value is not broadcastable with batch_shape+event_shape: torch.Size([64, 12]) vs torch.Size([64]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-fd8890f06a61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m plot_all(*run_experiment(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModelConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mvalue_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mValueConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'hybrid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-38e2f911e4aa>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(model, num_episodes, eps, value_function, training)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                     \u001b[0;32mand\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlast_training\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_interval\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m                     \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_memory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-cb4300196ed6>\u001b[0m in \u001b[0;36moptimize_model\u001b[0;34m(model, optimizer, memory, batch_size)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'optimize_model() {action_batch.shape}\\n{reward_batch}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_batch\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# \"-\" because it was built to work with gradient descent, but we are using gradient ascent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'optimize_model() {log_probs} {reward_batch}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/distributions/categorical.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_pmf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m_validate_sample\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m                 raise ValueError('Value is not broadcastable with batch_shape+event_shape: {} vs {}.'.\n\u001b[0m\u001b[1;32m    267\u001b[0m                                  format(actual_shape, expected_shape))\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Value is not broadcastable with batch_shape+event_shape: torch.Size([64, 12]) vs torch.Size([64])."
     ]
    }
   ],
   "source": [
    "plot_all(*run_experiment(\n",
    "    model=ModelConfig(name='linear'),\n",
    "    value_function=ValueConfig(name='hybrid', gamma=[0.0, 0.0, 200]),\n",
    "    eps=[0.0, 0.0, 400],\n",
    "    num_episodes=150,\n",
    "    training=TrainConfig(train_interval=16, batch_size=64, clear_memory=False, lr=0.07)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bbee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all(*run_experiment(\n",
    "    model=ModelConfig(name='linear'),\n",
    "    value_function=ValueConfig(name='hybrid', gamma=[0.0, 0.0, 200]),\n",
    "    eps=[0.0, 0.0, 400],\n",
    "    num_episodes=150,\n",
    "    training=TrainConfig(train_interval=16, batch_size=64, clear_memory=False, lr=0.03)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754258f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all(*run_experiment(\n",
    "    model=ModelConfig(name='linear'),\n",
    "    value_function=ValueConfig(name='hybrid', gamma=[0.0, 0.0, 200]),\n",
    "    eps=[0.0, 0.0, 400],\n",
    "    num_episodes=150,\n",
    "    training=TrainConfig(train_interval=16, batch_size=64, clear_memory=False, lr=0.1)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d116eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all(*run_experiment(\n",
    "    model=ModelConfig(name='linear'),\n",
    "    value_function=ValueConfig(name='hybrid', gamma=[0.3, 0.3, 200]),\n",
    "    eps=[0.0, 0.0, 400],\n",
    "    num_episodes=150,\n",
    "    training=TrainConfig(train_interval=16, batch_size=64, clear_memory=False, lr=0.07)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1b6a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all(*run_experiment(\n",
    "    model=ModelConfig(name='linear'),\n",
    "    value_function=ValueConfig(name='hybrid', gamma=[0.6, 0.6, 200]),\n",
    "    eps=[0.0, 0.0, 400],\n",
    "    num_episodes=150,\n",
    "    training=TrainConfig(train_interval=16, batch_size=64, clear_memory=False, lr=0.07)\n",
    "    ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
