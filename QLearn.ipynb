{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8ebc590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, pathlib, random, time\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "\n",
    "from environment import Env, validate_against_hint, load_word_lists, construct_word_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae1b8e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = construct_word_df(*load_word_lists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "895b599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def word_to_action(word, guesses, history):\n",
    "    return dfword_to_action((word, df.loc[word]), guesses, history)\n",
    "    \n",
    "def dfword_to_action(dfword, guesses, history):\n",
    "    #the action is going to be a word that we will submit next\n",
    "    #for the purposes of feeding into the model, we will represent the action word as:\n",
    "    #  how many of the entries in the hint history this word conforms to\n",
    "    #  how many untried letters it gives us\n",
    "    #  the number of uniq letters in the word\n",
    "    #  the frequency of the letters in the word\n",
    "    #  whether or not the word is in the guess list (as opposed to the target list)\n",
    "    word = dfword[0]\n",
    "    dfword = dfword[1]\n",
    "    \n",
    "    if guesses:\n",
    "        conforms_to_history = sum([int(validate_against_hint(word,g,history[i])) for i,g in enumerate(guesses)]) / len(guesses)\n",
    "    else: # we haven't made any guess yet, so this must conform\n",
    "        conforms_to_history = 1.0\n",
    "    num_untried_letters = len(set(word) - set(''.join(guesses))) / 5 #normalise to 1\n",
    "    action = np.array([conforms_to_history, num_untried_letters, dfword['freq_score'], dfword['uniq_score'], dfword['is_guess_word']])\n",
    "    \n",
    "    #if word == 'aargh':\n",
    "    #    print(f'recons', action, history, guesses)\n",
    "    return action   \n",
    "    \n",
    "\n",
    "def construct_action_vectors_global(arg): #guesses, history, start_idx, end_idx):\n",
    "    st = time.time()\n",
    "    guesses, history, start_idx, end_idx = arg\n",
    "    #print(guesses, history, start_idx, end_idx)\n",
    "    ret = np.array([dfword_to_action(dfword, guesses, history) for dfword in df.iloc[start_idx:end_idx].iterrows()])\n",
    "    #print(f'construct_actions_global took {time.time() - st}')\n",
    "    return ret\n",
    "           \n",
    "def construct_action_vectors(guesses, history, df):\n",
    "        return np.array([dfword_to_action(dfword, guesses, history) for dfword in df.iterrows()])\n",
    "    \n",
    "NUM_PROCESSES = mp.cpu_count() - 1\n",
    "def construct_action_vectors_mp(guesses, history, df):\n",
    "        grp_lst_args = []\n",
    "        grp_guesses = [guesses] * NUM_PROCESSES\n",
    "        grp_history = [history] * NUM_PROCESSES\n",
    "        \n",
    "        chunk_size = int(len(df) / NUM_PROCESSES) + 1\n",
    "        start_offsets = list(range(0, len(df), chunk_size))\n",
    "        end_offsets = start_offsets[1:] + [len(df)]\n",
    "        grp_lst_args = list(zip(grp_guesses, grp_history, start_offsets, end_offsets))\n",
    "        \n",
    "        #print(grp_lst_args)\n",
    "        pool = mp.Pool(processes=NUM_PROCESSES)\n",
    "        results = pool.map(construct_action_vectors_global, grp_lst_args)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        return np.concatenate(results)\n",
    "    \n",
    "    \n",
    "def construct_state_vector(guesses, history):\n",
    "        #print(history)\n",
    "        #so the state is going to be:\n",
    "            #  The number of green locations we know\n",
    "            #  The number of other letters we know to be in the word\n",
    "            #  The sequence number of the guess (1st guess, 2nd guess etc.)\n",
    "\n",
    "        #the number of locations which were green at some point in the history\n",
    "        num_green_locs = np.count_nonzero(history.max(axis=0) == 2)\n",
    "\n",
    "        green_chars = [guesses[x][y] for x,y in np.argwhere(history == 2) ]\n",
    "        orange_chars = [guesses[x][y] for x,y in np.argwhere(history == 1) ]\n",
    "        black_chars = [guesses[x][y] for x,y in np.argwhere(history == 0) ]\n",
    "        num_other_letters = len(set(orange_chars) - set(green_chars))\n",
    "        num_black_letters = len(set(black_chars))\n",
    "\n",
    "        sequence_number = int(history.size / 5)\n",
    "        #print(f'construct_state() with seqno {sequence_number}')\n",
    "\n",
    "        sequence_number_onehot = np.zeros(Env.num_guesses)\n",
    "        sequence_number_onehot[sequence_number] = 1.0\n",
    "        return np.concatenate((np.array([num_green_locs, num_other_letters, num_black_letters])/5, sequence_number_onehot))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0339f14",
   "metadata": {},
   "source": [
    "   \n",
    "so the state is going to be:\n",
    "* The number of green locations we know\n",
    "*  The number of other letters we know to be in the word\n",
    "*  The number of letters we know to not be in the word\n",
    "*  The sequence number of the guess (1st guess, 2nd guess etc.)\n",
    "\n",
    "the action is going to be a word that we will submit next\n",
    "for the purposes of feeding into the model, we will represent the action word as:\n",
    "*  whether or not it conforms to the hint history\n",
    "*  how many new letters it gives us\n",
    "*  the number of uniq letters in the word\n",
    "*  the frequency of the letters in the word\n",
    "\n",
    "the reward is going to be:\n",
    "*  the score improvement (if any) gained on the last guess\n",
    "*  the score will be calculated as 2 * num_green_letters + num_orange_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9dd99de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "#plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "832e7980",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def clear(self):\n",
    "        self.memory.clear()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb9ed4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get number of actions from gym action space\n",
    "#n_actions = env.action_space.n\n",
    "n_action_features = 5\n",
    "n_state_features = 9\n",
    "n_input_features = n_action_features + n_state_features\n",
    "\n",
    "\n",
    "def select_action(policy_net, state, actions, eps_threshold):\n",
    "    sample = random.random()\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            #now combine the state (shape 3,) and action (shape 5, n) into one input array (shape 8,n)\n",
    "            #first expand the state so that it is shape 3,1\n",
    "            #then repeat it to 3,n\n",
    "            states = np.repeat(np.expand_dims(state, 0), actions.shape[0], axis=0)\n",
    "            #print(f'states shape {states.shape} actions shape {actions.shape}')\n",
    "            #then concatenate to 8,n\n",
    "            state_actions = np.concatenate((states, actions), axis=1)\n",
    "            # policy_net(state_action) will return a single value estimate for each state/action row\n",
    "            # so, probably shape (1,n)\n",
    "            # Then return the index which has the max value\n",
    "            \n",
    "            estimate = policy_net(torch.tensor(state_actions, device=device, dtype=torch.float))\n",
    "            #print(f'ESTIMATE>>>{estimate.__class__} {estimate.shape} {estimate} {estimate.max(0).indices.item()}<<<')\n",
    "            return estimate.max(0).indices.item()\n",
    "    else:\n",
    "        randindex = random.randrange(len(actions))\n",
    "        print(f'returning random index {randindex}')\n",
    "        return randindex #torch.tensor([[randindex]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "\n",
    "def plot_values(vals, axes=['duration', 'episode']):\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel(axes[1])\n",
    "    plt.ylabel(axes[0])\n",
    "    plt.plot(np.array(vals))\n",
    "    # Take 20 episode averages and plot them too\n",
    "    window_width = 20\n",
    "    if len(vals) >= window_width:\n",
    "        cumsum_vec = np.cumsum(np.insert(vals, 0, 0)) \n",
    "        ma_vec = (cumsum_vec[window_width:] - cumsum_vec[:-window_width]) / window_width\n",
    "        plt.plot(np.insert(ma_vec, 0, [None]*int(window_width/2)))\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    #if is_ipython:\n",
    "    #    display.clear_output(wait=True)\n",
    "    #    display.display(plt.gcf())\n",
    "    \n",
    "def plot_all(episode_durations, episode_rewards, losses, epsilons, gammas):\n",
    "    plot_values(episode_durations, axes=['duration', 'episode'])\n",
    "    plot_values(episode_rewards, axes=['reward', 'episode'])\n",
    "    if losses: plot_values(losses, axes=['loss', 'step'])\n",
    "    if epsilons: plot_values(epsilons, axes=['epsilon', 'step'])\n",
    "    if gammas: plot_values(gammas, axes=['gamma', 'step'])\n",
    "    #plt.ioff()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ae28e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, inputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(inputs, 20)\n",
    "        self.fc2 = nn.Linear(20, 16)\n",
    "        self.fc3 = nn.Linear(16, 20)\n",
    "        self.head = nn.Linear(20, 1)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c642286",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearQ(nn.Module):\n",
    "\n",
    "    def __init__(self, inputs):\n",
    "        super(LinearQ, self).__init__()\n",
    "        self.head = nn.Linear(inputs, 1)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14c1200d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(model, optimizer, memory, batch_size=128):\n",
    "\n",
    "    transitions = memory.sample(batch_size)\n",
    "\n",
    "    #for tr in transitions:\n",
    "    #    print(list(tr.state), list(tr.action), tr.reward)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    state_batch = np.stack([tr.state for tr in transitions])\n",
    "    action_batch = np.stack([tr.action for tr in transitions])\n",
    "      \n",
    "    reward_batch = np.stack([tr.reward for tr in transitions])\n",
    "    state_action_batch = np.concatenate((state_batch, action_batch), axis=1)\n",
    "    \n",
    "    #print(reward_batch)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_value_estimates = model(torch.tensor(state_action_batch, device=device, dtype=torch.float))\n",
    "    #print(f'ESTIMATE>>>{estimate.__class__} {estimate.shape} {estimate} {estimate.max(0).indices.item()}<<<')\n",
    "       \n",
    "    expected_state_action_values = torch.tensor(reward_batch, device=device, dtype=torch.float)\n",
    "    # Compute Huber loss\n",
    "    print('loss shapes')\n",
    "    print(state_action_value_estimates.shape)\n",
    "    print(expected_state_action_values.shape)\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_value_estimates, expected_state_action_values)\n",
    "    \n",
    "    print(f'loss {loss}')\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    #for param in model.parameters():\n",
    "        #param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n",
    "\n",
    "class TrainConfig():\n",
    "    def __init__(self, train_interval=128, batch_size=128, clear_memory=False, lr=0.01):\n",
    "        self.train_interval = train_interval\n",
    "        self.batch_size = batch_size\n",
    "        self.clear_memory = clear_memory\n",
    "        self.lr = lr\n",
    "        \n",
    "class ValueConfig():\n",
    "    def __init__(self, name='reward', gamma=[0.9, 0.05, 200]):\n",
    "        self.name = name\n",
    "        self.gamma = gamma\n",
    "        \n",
    "class ModelConfig():\n",
    "    def __init__(self, name='naive', startword=None, target_list_only=None):\n",
    "        self.name = name\n",
    "        self.startword = startword\n",
    "        self.target_list_only = target_list_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71c62073",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "def run_experiment(model=ModelConfig(name='naive', startword=None, target_list_only=False),\n",
    "                   num_episodes=128,\n",
    "                   eps=[0.9, 0.05, 200],\n",
    "                   value_function=ValueConfig(name='reward',gamma=[0.0, 1.0, 200]),\n",
    "                   training=TrainConfig(clear_memory=False, batch_size=128, train_interval=128),\n",
    "                   seed=0,\n",
    "                   run_test=False):\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    EPS_START = eps[0]\n",
    "    EPS_END = eps[1]\n",
    "    EPS_DECAY = eps[2]\n",
    "    GAMMA_START, GAMMA_END, GAMMA_DECAY = value_function.gamma\n",
    "    env = Env(df)\n",
    "    memory = ReplayMemory(10000)\n",
    "    starting_actions = construct_action_vectors(env.guesses, env.history, env.df)\n",
    "    starting_state = construct_state_vector(env.guesses, env.history)\n",
    "\n",
    "    steps_done = 0\n",
    "    last_training = 0\n",
    "    losses = []\n",
    "    episode_rewards = []\n",
    "    episode_durations = []\n",
    "    epsilons = []\n",
    "    gammas = []\n",
    "    \n",
    "    if model.name == 'linear':\n",
    "        policy_net = LinearQ(n_input_features).to(device)\n",
    "        optimizer = optim.RMSprop(policy_net.parameters(), lr=training.lr)\n",
    "    else:\n",
    "        policy_net = DQN(n_input_features).to(device)\n",
    "        optimizer = optim.RMSprop(policy_net.parameters(), lr=training.lr)\n",
    "\n",
    "    print(f'pn params {list(policy_net.parameters())}')\n",
    "    for i_episode in range(num_episodes):\n",
    "        # Initialize the environment and state\n",
    "        env.reset()\n",
    "        print(f'=========================episode {i_episode} {env.target}======================')\n",
    "\n",
    "        episode_memory = []\n",
    "        state = starting_state\n",
    "        actions = starting_actions\n",
    "        guesses = []\n",
    "        for t in count():\n",
    "            eps = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "            GAMMA = GAMMA_END + (GAMMA_START - GAMMA_END) * math.exp(-1. * steps_done / GAMMA_DECAY)\n",
    "            \n",
    "            epsilons.append(eps)\n",
    "            #print(epsilons)\n",
    "            gammas.append(GAMMA)\n",
    "            steps_done += 1\n",
    "            # Select and perform an action\n",
    "            #print(state)\n",
    "            action_idx = select_action(policy_net, state, actions, eps)\n",
    "            selected_action = actions[action_idx]\n",
    "            #print(action_idx, selected_action)\n",
    "            guesses.append(env.word_from_index(action_idx))\n",
    "            print(f'------guess {t} {action_idx} {guesses[-1]} {selected_action}-------')\n",
    "            \n",
    "            history, reward, done = env.step_by_index(action_idx)\n",
    "            #here next_state == env.history\n",
    "            if not done:\n",
    "                next_state = construct_state_vector(guesses, history)\n",
    "                actions = construct_action_vectors_mp(guesses, history, env.df)\n",
    "            \n",
    "            print(f'reward {reward} done {done} ')\n",
    "            #reward = np.array([reward])\n",
    "\n",
    "            # Store the transition in memory\n",
    "            #memory.push(state, selected_action, reward)\n",
    "            episode_memory.append([state, selected_action, reward])\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                episode_durations.append(t + 1)\n",
    "                scores = history.sum(axis=1) # calc the score for each guess from the history\n",
    "                episode_reward = sum(tr[2] for tr in episode_memory)\n",
    "                print(f'episode {i_episode} finished.  reward {episode_reward}  eps {eps}  gamma {GAMMA}  steps {steps_done}  memory {len(memory)}')\n",
    "                episode_rewards.append(episode_reward)\n",
    "                # reward[i] = max(0, scores[i] - max(scores[0:i-1]))\n",
    "                \n",
    "                for i,tr in enumerate(episode_memory):\n",
    "                    if i == 0:\n",
    "                        score = scores[i]\n",
    "                    else:\n",
    "                        score = max(0, scores[i] - scores[0:i].max())\n",
    "                    print(f'{guesses[i]} {history[i]} {score}')    \n",
    "                \n",
    "                    memory.push(tr[0], tr[1], [score])\n",
    "                    \n",
    "                # If we have gathered enough data, Perform one step of the optimization (on the policy network)\n",
    "                if len(memory) >= training.batch_size \\\n",
    "                    and steps_done - last_training > training.train_interval:\n",
    "                    loss = optimize_model(policy_net, optimizer, memory, batch_size=training.batch_size)\n",
    "                    losses.append(loss)\n",
    "                    if training.clear_memory: memory.clear()\n",
    "                    last_training = steps_done\n",
    "                #plot_durations()\n",
    "                break\n",
    "\n",
    "    print('Complete')\n",
    "    \n",
    "    if run_test:\n",
    "        performance_hist = [0] * (1 + env.num_guesses)\n",
    "        for e in env.foreach_target_word():\n",
    "            state = starting_state\n",
    "            actions = starting_actions\n",
    "            done = False\n",
    "            reward = 0\n",
    "            num_guesses = 0\n",
    "            while not done:\n",
    "                action_idx = select_action(policy_net, state, actions, eps)\n",
    "                selected_action = actions[action_idx]\n",
    "                guesses.append(env.word_from_index(action_idx))\n",
    "                _, reward, done = env.step_by_index(action_idx)\n",
    "                num_guesses += 1\n",
    "            \n",
    "            if num_guesses == 6 and reward == -1:\n",
    "                num_guesses = 0\n",
    "            print(f'{e.target} {num_guesses}')    \n",
    "            performance_hist[num_guesses] += 1\n",
    "            \n",
    "        for i,p in enumerate(performance_hist):\n",
    "            print(f'{i}: {p}')\n",
    "    \n",
    "    return episode_durations, episode_rewards, losses, epsilons, gammas\n",
    "\n",
    "#env.render()\n",
    "#env.close()\n",
    "#plt.ioff()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0667e55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_all(*run_experiment(\n",
    "    model=ModelConfig(name='linear'),\n",
    "    value_function=ValueConfig(name='hybrid', gamma=[0.0, 0.0, 200]),\n",
    "    eps=[0.0, 0.0, 400],\n",
    "    num_episodes=0,\n",
    "    training=TrainConfig(train_interval=16, batch_size=64, clear_memory=False, lr=0.07),\n",
    "    seed=0,\n",
    "    run_test=True\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f94916dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pn params [Parameter containing:\n",
      "tensor([[-0.0020,  0.1434, -0.2200, -0.1967, -0.1029,  0.0717, -0.0053,  0.2119,\n",
      "         -0.0237,  0.0707, -0.0808, -0.0525, -0.2553, -0.1770]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.1102], requires_grad=True)]\n",
      "=========================episode 0 lorry======================\n",
      "------guess 0 12621 jumpy [1.         1.         0.25985401 0.         0.        ]-------\n",
      "reward -1 done False \n",
      "------guess 1 11041 gawky [1.         0.8        0.34387672 0.         0.        ]-------\n",
      "reward -1 done False \n",
      "------guess 2 12621 jumpy [0.5        0.         0.25985401 0.         0.        ]-------\n",
      "reward -1 done False \n",
      "------guess 3 11041 gawky [0.66666667 0.         0.34387672 0.         0.        ]-------\n",
      "reward -1 done False \n",
      "------guess 4 12621 jumpy [0.5        0.         0.25985401 0.         0.        ]-------\n",
      "reward -1 done False \n",
      "------guess 5 11041 gawky [0.6        0.         0.34387672 0.         0.        ]-------\n",
      "reward -1 done True \n",
      "episode 0 finished.  reward -6  eps 0.0  gamma 0.0  steps 6  memory 0\n",
      "jumpy [0. 0. 0. 0. 2.] 2.0\n",
      "gawky [0. 0. 0. 0. 2.] 0\n",
      "jumpy [0. 0. 0. 0. 2.] 0\n",
      "gawky [0. 0. 0. 0. 2.] 0\n",
      "jumpy [0. 0. 0. 0. 2.] 0\n",
      "gawky [0. 0. 0. 0. 2.] 0\n",
      "=========================episode 1 rebar======================\n",
      "------guess 0 12621 jumpy [1.         1.         0.25985401 0.         0.        ]-------\n",
      "reward -1 done False \n",
      "------guess 1 12621 jumpy [0.         0.         0.25985401 0.         0.        ]-------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-77:\n",
      "Process ForkPoolWorker-69:\n",
      "Process ForkPoolWorker-73:\n",
      "Process ForkPoolWorker-70:\n",
      "Process ForkPoolWorker-74:\n",
      "Process ForkPoolWorker-68:\n",
      "Process ForkPoolWorker-75:\n",
      "Process ForkPoolWorker-76:\n",
      "Process ForkPoolWorker-67:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"<ipython-input-3-8e83604febd0>\", line 31, in construct_action_vectors_global\n",
      "    ret = np.array([dfword_to_action(dfword, guesses, history) for dfword in df.iloc[start_idx:end_idx].iterrows()])\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 48, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 48, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"<ipython-input-3-8e83604febd0>\", line 31, in construct_action_vectors_global\n",
      "    ret = np.array([dfword_to_action(dfword, guesses, history) for dfword in df.iloc[start_idx:end_idx].iterrows()])\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"<ipython-input-3-8e83604febd0>\", line 31, in <listcomp>\n",
      "    ret = np.array([dfword_to_action(dfword, guesses, history) for dfword in df.iloc[start_idx:end_idx].iterrows()])\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"<ipython-input-3-8e83604febd0>\", line 31, in <listcomp>\n",
      "    ret = np.array([dfword_to_action(dfword, guesses, history) for dfword in df.iloc[start_idx:end_idx].iterrows()])\n",
      "  File \"<ipython-input-3-8e83604febd0>\", line 16, in dfword_to_action\n",
      "    conforms_to_history = sum([int(validate_against_hint(word,g,history[i])) for i,g in enumerate(guesses)]) / len(guesses)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 48, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 48, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 48, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"<ipython-input-3-8e83604febd0>\", line 31, in construct_action_vectors_global\n",
      "    ret = np.array([dfword_to_action(dfword, guesses, history) for dfword in df.iloc[start_idx:end_idx].iterrows()])\n",
      "  File \"<ipython-input-3-8e83604febd0>\", line 16, in dfword_to_action\n",
      "    conforms_to_history = sum([int(validate_against_hint(word,g,history[i])) for i,g in enumerate(guesses)]) / len(guesses)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "plot_all(*run_experiment(\n",
    "    model=ModelConfig(name='linear'),\n",
    "    value_function=ValueConfig(name='hybrid', gamma=[0.0, 0.0, 200]),\n",
    "    eps=[0.0, 0.0, 400],\n",
    "    num_episodes=150,\n",
    "    training=TrainConfig(train_interval=16, batch_size=64, clear_memory=False, lr=0.07),\n",
    "    seed=0\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4332c0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all(*run_experiment(\n",
    "    model=ModelConfig(name='linear'),\n",
    "    value_function=ValueConfig(name='hybrid', gamma=[0.0, 0.0, 200]),\n",
    "    eps=[0.0, 0.0, 400],\n",
    "    num_episodes=150,\n",
    "    training=TrainConfig(train_interval=16, batch_size=64, clear_memory=False, lr=0.07),\n",
    "    seed=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cf6167",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
